{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32eebf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fd3c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1d0668",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''LOADING TRAIN AND TEST DATA SET'''\n",
    "(X_train,Y_train),(X_test,Y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02d9aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Shape of input train data is : {X_train.shape}')\n",
    "print(f'Shape of input test data is : {X_test.shape}')\n",
    "print(f'Shape of output train data is : {Y_train.shape}')\n",
    "print(f'Shape of output test data is : {Y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14821b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Finding image of each category'''\n",
    "categories=[]\n",
    "images_list=[]\n",
    "attributes={0:'T-shirt',1:'Trouser',2:'Pullover',3:'Skirt',4:'Overcoat',5:'Sandal',\n",
    "            6:'Shirt',7:'Sneakers',8:'Bag',9:'Ankle boot'}\n",
    "title=list(attributes.values())\n",
    "print(f'The various categorical variables in the data are : {title}')\n",
    "for i in range(10):\n",
    "    categories.append(i)\n",
    "for i in range(Y_train.shape[0]):\n",
    "               if Y_train[i]==categories[0]:\n",
    "                    images_list.append(X_train[i])\n",
    "                    del(categories[0])\n",
    "                    if len(categories)==0:\n",
    "                        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdf6943",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''Plotting image of each category'''\n",
    "for i in range(len(attributes)):\n",
    "    print(f'Numerical value equivalent to categorical variable : {Y_train[i]}')\n",
    "    plt.imshow(images_list[i], cmap=plt.get_cmap())\n",
    "    plt.title(attributes[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe1b04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"CS6910_DL_ASS1\", name='Assignment_1_sample_imgs')\n",
    "'''Displaying sample images in wandb'''\n",
    "wandb.log({\"Sample Image from each class\": [wandb.Image(image, caption=caption) for image, \n",
    "                                            caption in zip(images_list,title)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb53d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''CHANGING THE SHAPE OF INPUT DATA'''\n",
    "x_train=np.zeros((60000,784))\n",
    "for i in range(X_train.shape[0]):\n",
    "    a=X_train[i].reshape(1,784)\n",
    "    x_train[i]=a\n",
    "print('Train data',x_train.shape)\n",
    "x_test=np.zeros((10000,784))\n",
    "for i in range(X_test.shape[0]):\n",
    "    a=X_test[i].reshape(1,784)\n",
    "    x_test[i]=a\n",
    "print('Test data',x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3879212",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''CONVERTING OUTPUT DATA INTO ONE HOT VECTOR FORM'''\n",
    "a = np.max(Y_train)+1\n",
    "y_train=np.zeros((Y_train.shape[0],a))\n",
    "for i in range(Y_train.shape[0]):\n",
    "    for j in range(a):\n",
    "        if Y_train[i]==j:\n",
    "            y_train[i,j]=1\n",
    "print('Train data',y_train.shape)\n",
    "y_test=np.zeros((Y_test.shape[0],a))\n",
    "for i in range(Y_test.shape[0]):\n",
    "    for j in range(a):\n",
    "        if Y_test[i]==j:\n",
    "            y_test[i,j]=1\n",
    "#print(y_test[0,:])\n",
    "print('Test data',y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a69aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''CREATING VALIDATION DATA SET'''\n",
    "'''The input data is split into train and validation data where validation comprises of 10% of the data.'''\n",
    "\n",
    "x_train,x_val,y_train,y_val = train_test_split(x_train,y_train,test_size=0.10,random_state=10)\n",
    "x_train=x_train.T/255\n",
    "y_train=y_train.T\n",
    "x_test=x_test.T/255\n",
    "y_test=y_test.T\n",
    "x_val=x_val.T/255\n",
    "y_val=y_val.T\n",
    "print(f'The shape of train data is:{x_train.shape}')\n",
    "print(f'The shape of test data is:{x_test.shape}')\n",
    "print(f'The shape of validation data is:{x_val.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1897ff7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''ACTIVATION FUNCTIONS'''\n",
    "\n",
    "'''input: zl = w(l)*a(l-1) + b(l) where l is the lth Layer.The various activation functions implemented here are \n",
    "sigmoid,tanh,ReLu and Identity functions.'''\n",
    "\n",
    "\n",
    "#SIGMOID FUNCTION\n",
    "def sigmoid_function(z):\n",
    "    h=1./(1.+np.exp(-z))\n",
    "    \n",
    "    return h\n",
    "\n",
    "#TANH FUNCTION\n",
    "def tanh_function(z):\n",
    "    h=np.tanh(z)\n",
    "    \n",
    "    return h\n",
    "\n",
    "#RELU FUNCTION\n",
    "def relu_function(z):\n",
    "    h=np.maximum(z,0)\n",
    "    \n",
    "    return h\n",
    "    \n",
    "\n",
    "#IDENTITY FUNCTION\n",
    "def identity_function(z):\n",
    "    \n",
    "    return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b370971a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OUTPUT LAYER FUNCTION\n",
    "'''The given problem is a multi-class classification problem.So,we use softmax function for the output layer(L)\n",
    "    Z(L) = W(L)*A(L-1) + B(L) where Lth layer is the output layer.'''\n",
    "\n",
    "\n",
    "#SOFTMAX FUNCTION\n",
    "'''OUTPUT LAYER FUNCTION'''\n",
    "\n",
    "def softmax_function(z):\n",
    "    #z=z-np.max(z,axis=0,keepdims=True) # to avoid Nan or division by zero errors\n",
    "    h = np.exp(z)/np.sum(np.exp(z), axis=0)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff00b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''DERIVATIVE FUNCTIONS'''\n",
    "\n",
    "'''These are the derivatives of the corresponding activation functions, which is used in backpropagation to find\n",
    "the derivative of activation functions.'''\n",
    "\n",
    "#DERIVATIVE OF SIGMOID FUNCTION\n",
    "def sigmoid_function_dash(z):\n",
    "    h = sigmoid_function(z)\n",
    "    \n",
    "    return h*(1-h)\n",
    "\n",
    "\n",
    "#DERIVATIVE OF TANH FUNCTION\n",
    "def tanh_function_dash(z):\n",
    "    h=tanh_function(z)\n",
    "    \n",
    "    return 1-(h)**2\n",
    "\n",
    "\n",
    "#DERIVATIVE OF RELU FUNCTION\n",
    "def relu_function_dash(z):\n",
    "    return 1*(z>0)\n",
    "    \n",
    "#DERIVATIVE OF IDENTITY FUNCTION\n",
    "def identity_function_dash(z):\n",
    "    h = identity_function(z)\n",
    "    \n",
    "    return np.ones(z.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83307593",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''SOFTMAX DERIVATIVE'''\n",
    "def softmax_dash(Z):\n",
    "    h= softmax(z) * (1-softmax(z))\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8b063f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CROSS ENTROPY FUNCTION(DERIVATIVE OF OUTPUT LAYER)\n",
    "def cross_entropy_function(y,ycap,w,lambd):\n",
    "    '''This function is called as categorical cross entropy function.\n",
    "       input: Y:actual value of output\n",
    "              YCAP:predicted value of output\n",
    "              lambd:Regularisation parameter(L2 Rregularization is used here)'''\n",
    "    \n",
    "    #ycap = np.clip(ycap, 1e-12, 1.0 - 1e-12) #to avoid Nan error \n",
    "    m=y.shape[1]\n",
    "    cost=-(1/m)*np.sum(y*np.log(ycap))\n",
    "    regularization_cost=0\n",
    "    for i in range(len(w)):\n",
    "        regularization_cost += (lambd/(2*m))*np.sum(np.square(w[i]))\n",
    "        \n",
    "    return cost+regularization_cost\n",
    "     \n",
    "        \n",
    "#MEAN SQUARED ERROR FUNCTION\n",
    "def mean_squared_error_function(y,ycap,w,lambd):\n",
    "    '''input: Y:actual value of output\n",
    "              YCAP:predicted value of output\n",
    "              lambd:Regularisation parameter(L2 Rregularization is used here)'''   \n",
    "    ycap = np.clip(ycap, 1e-12, 1.0 - 1e-12)\n",
    "    m = y.shape[1]\n",
    "    mean_square_error = (1/m)*np.sum((y-ycap)**2)\n",
    "    reg_cost=0\n",
    "    for i in range(len(w)):\n",
    "        reg_cost += (lambd/(2*m))*np.sum(w[i]**2)\n",
    "    return mean_square_error + reg_cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c4bdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INITIALISE PARAMETERS\n",
    "\n",
    "'''input:  Layer_attributes is a list consisting of number of \n",
    "    neurons in each layer. Here,input layer is considered as 0th Layer, output layer is considered as Lth layer\n",
    "    and the layers from 1 to (L-1) are considered as hidden layers.Therefore, layer-attributes consists of (L+1)\n",
    "    values. The methods used here to initialise the values of parameters are Random and Xavier Initialisations.'''\n",
    "\n",
    "def random_initialization(layer_attributes):\n",
    "    \n",
    "    L=len(layer_attributes)-1\n",
    "    W=[]\n",
    "    B=[]\n",
    "    np.random.seed(10)\n",
    "    for i in range(1,L+1):\n",
    "        weight_i = np.random.uniform(-1,1,(layer_attributes[i],layer_attributes[i-1]))\n",
    "        bias_i=np.zeros((layer_attributes[i],1))\n",
    "        #bias_i=np.full((layer_attributes[i],1),0.01) # in case of relu function to avoid vanishing gradient\n",
    "        W.append(weight_i)\n",
    "        B.append(bias_i)\n",
    "        \n",
    "    return W,B\n",
    "\n",
    "def xavier_initialization(layer_attributes):\n",
    "    \n",
    "    L=len(layer_attributes)-1\n",
    "    W=[]\n",
    "    B=[]\n",
    "    for i in range(1,L+1):\n",
    "        lim = np.sqrt(6/(i+(i-1)))\n",
    "        weight_i = np.random.uniform(-lim,lim,(layer_attributes[i],layer_attributes[i-1]))\n",
    "        bias_i=np.zeros((layer_attributes[i],1))\n",
    "        W.append(weight_i)\n",
    "        B.append(bias_i)\n",
    "        \n",
    "    return W,B\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89328db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FORWARD PROPAGATION\n",
    "def forward_propagation(x,w,b,activation='sigmoid_function'):\n",
    "    \n",
    "    '''Forward propagation is used to find the predicted value of output and cost function by going forward,starting from \n",
    "    input layer until the output layer.We calculate the pre-activation and activation values and returns the latter after each\n",
    "    layer. The input parameters taken are input data set,weights and bias value, and activation function to be used where the \n",
    "    default is set as sigmoid function. Softmax function is used to find the values at the output layer.\n",
    "    Here,z is the linear part and a is the non-linear part(activation function) of a neuron.'''\n",
    "    A=[]\n",
    "    Z=[]\n",
    "    length=len(w)\n",
    "    #Hidden layers\n",
    "    A.append(x)\n",
    "    for i in range(length-1):\n",
    "        z_i=np.dot(w[i],A[-1])+b[i]\n",
    "        Z.append(z_i)\n",
    "        if activation =='sigmoid_function':\n",
    "            a_i = sigmoid_function(z_i)\n",
    "            A.append(a_i)\n",
    "        elif activation=='tanh_function':\n",
    "            a_i = tanh_function(z_i)\n",
    "            A.append(a_i)\n",
    "        elif activation == 'relu_function':\n",
    "            a_i = relu_function(z_i)\n",
    "            A.append(a_i)\n",
    "        elif activation == 'identity_function':\n",
    "            a_i = identity_function(z_i)\n",
    "            A.append(a_i)\n",
    "    #output layer\n",
    "    z_l = np.dot(w[-1],A[-1]) + b[-1]\n",
    "    a_l = softmax_function(z_l)\n",
    "    A.append(a_l)\n",
    "    Z.append(z_l)\n",
    "\n",
    "    return Z,A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9e67bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BACK PROPAGATION\n",
    "def back_propagation(A,y,W,B,Z,lambd,activation='sigmoid_function',loss='cross_entropy_function'):\n",
    "    \n",
    "    '''Back propagation is used to find the derivatives of each weights and biases at each layers by starting \n",
    "    from the output layer and travelling backwards.We find the derivatives wrto ouput layer,wrto hidden layer and eventually\n",
    "    wrto weights and biases;dw=dJ/dw,db=dJ/db,dz=dJ/dz.'''\n",
    "    m=y.shape[1]\n",
    "    L=len(W)\n",
    "    dW=[]\n",
    "    dB=[]\n",
    "    dZ=[]\n",
    "    #Output Layer\n",
    "    if loss=='cross_entropy_function':\n",
    "        dZ.append(A[-1]-y)\n",
    "        dB.append((1/m)*np.sum(dZ[-1],axis=1,keepdims=True))\n",
    "        dW.append((1/m)*(np.dot(dZ[-1],A[-2].T))+(lambd/m)*W[-1])\n",
    "    elif loss=='mean_squared_error_function':\n",
    "        dZ.append((A[-1]-y)*A[-1]*(1-A[-1]))\n",
    "        dB.append((1/m)*np.sum(dZ[-1],axis=1,keepdims=True))\n",
    "        dW.append((1/m)*(np.dot(dZ[-1],A[-2].T))+(lambd/m)*W[-1])\n",
    "        \n",
    "   \n",
    "    #Hidden layers\n",
    "    l=L-1\n",
    "    while l >0:\n",
    "        if activation=='sigmoid_function':\n",
    "            dz_l = (1/m)*np.dot(W[l].T,dZ[-1])*sigmoid_function_dash(A[l])\n",
    "            db_l = (1/m)*np.sum(dz_l,axis=1,keepdims=True)\n",
    "            dw_l = (1/m)*np.dot(dz_l,A[l-1].T) + (lambd/m)*W[l-1]\n",
    "            dW.append(dw_l)\n",
    "            dB.append(db_l)\n",
    "            dZ.append(dz_l)\n",
    "        \n",
    "        elif activation == 'relu_function':\n",
    "            dz_l = (1/m)*np.dot(W[l].T,dZ[-1])*relu_function_dash(A[l])\n",
    "            db_l = (1/m)*np.sum(dz_l,axis=1,keepdims=True)\n",
    "            dw_l = (1/m)*np.dot(dz_l,A[l-1].T) + (lambd/m)*W[l-1]\n",
    "            dW.append(dw_l)\n",
    "            dB.append(db_l)\n",
    "            dZ.append(dz_l)\n",
    "            \n",
    "        elif activation=='tanh_function':\n",
    "            dz_l = (1/m)*np.dot(W[l].T,dZ[-1])*tanh_function_dash(A[l])\n",
    "            db_l = (1/m)*np.sum(dz_l,axis=1,keepdims=True)\n",
    "            dw_l = (1/m)*np.dot(dz_l,A[l-1].T) + (lambd/m)*W[l-1]\n",
    "            dW.append(dw_l)\n",
    "            dB.append(db_l)\n",
    "            dZ.append(dz_l)\n",
    "            \n",
    "        elif activation=='identity_function':\n",
    "            dz_l = (1/m)*np.dot(W[l].T,dZ[-1])*identity_function_dash(A[l])\n",
    "            db_l = (1/m)*np.sum(dz_l,axis=1,keepdims=True)\n",
    "            dw_l = (1/m)*np.dot(dz_l,A[l-1].T) + (lambd/m)*W[l-1]\n",
    "            dW.append(dw_l)\n",
    "            dB.append(db_l)\n",
    "            dZ.append(dz_l)\n",
    "            \n",
    "        l=l-1\n",
    "            \n",
    "    return dZ[::-1],dW[::-1],dB[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733191ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTIMIZERS\n",
    "def gradient_descent(W,B,dW,dB,learning_rate):\n",
    "    \n",
    "    '''Mini batch,vanilla and stochastic gradient descents can be performed for this basic \n",
    "    gradient descent variant.'''\n",
    "    \n",
    "    alpha=learning_rate\n",
    "    length=len(W)\n",
    "    for i in range(length):\n",
    "        W[i] = W[i] - alpha*dW[i]\n",
    "        B[i] = B[i] - alpha*dB[i]\n",
    "    return W,B\n",
    "\n",
    "def momentum_gradient_descent(w,b,dw,db,learning_rate,momentum,update_w,update_b):\n",
    "    \n",
    "    '''Nesterov accelarated gradient descent can also be implemented as a special case of this Momentum gradient\n",
    "    descent  where we find wlookahead before calculating gradients '''\n",
    "    \n",
    "    for i in range(len(w)):  \n",
    "        update_w[i] = (momentum*update_w[i]) + dw[i]\n",
    "        update_b[i] = (momentum*update_b[i]) + db[i]\n",
    "        w[i] = w[i] - (learning_rate*update_w[i])\n",
    "        b[i] = b[i] - (learning_rate*update_b[i])\n",
    "    return w,b,update_w,update_b\n",
    "\n",
    "def rms_prop(w,b,dw,db,learning_rate,beta,epsilon,v_t_w,v_t_b):\n",
    "    \n",
    "    '''RMSProp is an adaptive learning algorithm and the hyperparamteters used \n",
    "    here are learning_rate(alpha) and beta.'''\n",
    "    \n",
    "    for i in range(len(w)):\n",
    "        v_t_w[i] = beta*v_t_w[i] + (1-beta)*((dw[i])**2)\n",
    "        v_t_b[i] = beta*v_t_b[i] + (1-beta)*((db[i])**2)\n",
    "        w[i] = w[i] - (learning_rate/(np.sqrt(v_t_w[i]+epsilon)))*dw[i]\n",
    "        b[i] = b[i] - (learning_rate/(np.sqrt(v_t_b[i]+epsilon)))*db[i]\n",
    "        return w,b,v_t_w,v_t_b\n",
    "\n",
    "\n",
    "def adam(w,b,dw,db,learning_rate,beta1,beta2,epsilon,m_w,m_b,v_w,v_b,t):\n",
    "    \n",
    "    '''Adam(Adaptive moments) is one of the widely used adaptive learning gradient descent variant and it takes \n",
    "    beta1,beta2,learning_rate(alpha) as hyperparameters and also a small value epsilon to avoid zero division\n",
    "    errors.'''\n",
    "    \n",
    "    m_w_cap=[]\n",
    "    m_b_cap=[]\n",
    "    v_w_cap=[]\n",
    "    v_b_cap=[]\n",
    "    for k in range(len(w)):\n",
    "        upd_w=np.zeros((w[k].shape))\n",
    "        upd_b=np.zeros((b[k].shape))\n",
    "        m_w_cap.append(upd_w)\n",
    "        m_b_cap.append(upd_b)\n",
    "        v_w_cap.append(upd_w)\n",
    "        v_b_cap.append(upd_b)\n",
    "        \n",
    "    for i in range(len(w)):\n",
    "        m_w[i]=beta1*m_w[i]+(1-beta1)*dw[i]\n",
    "        m_b[i]=beta1*m_b[i]+(1-beta1)*db[i]\n",
    "        v_w[i]=beta2*v_w[i]+(1-beta2)*(dw[i]**2)\n",
    "        v_b[i]=beta2*v_b[i]+(1-beta2)*(db[i]**2)\n",
    "        m_w_cap[i] = (1/(1-math.pow(beta1,t)))*m_w[i]\n",
    "        m_b_cap[i] = (1/(1-math.pow(beta1,t)))*m_b[i]\n",
    "        v_w_cap[i] = (1/(1-math.pow(beta2,t)))*v_w[i]\n",
    "        v_b_cap[i] = (1/(1-math.pow(beta2,t)))*v_b[i]\n",
    "        w[i] = w[i] - (learning_rate/(np.sqrt(v_w_cap[i]+epsilon)))*m_w_cap[i]\n",
    "        b[i] = b[i] - (learning_rate/(np.sqrt(v_b_cap[i]+epsilon)))*m_b_cap[i]\n",
    "        \n",
    "    return w,b,m_w,m_b,v_w,v_b\n",
    "\n",
    "def nadam(w,b,dw,db,learning_rate,beta1,beta2,epsilon,m_w,m_b,v_w,v_b,t):\n",
    "    \n",
    "    '''NAdam(Nesterov Adam) is the nesterov gradient variant of Adam'''\n",
    "    \n",
    "    m_w_cap=[]\n",
    "    m_b_cap=[]\n",
    "    v_w_cap=[]\n",
    "    v_b_cap=[]\n",
    "    for k in range(len(w)):\n",
    "        upd_w=np.zeros((w[k].shape))\n",
    "        upd_b=np.zeros((b[k].shape))\n",
    "        m_w_cap.append(upd_w)\n",
    "        m_b_cap.append(upd_b)\n",
    "        v_w_cap.append(upd_w)\n",
    "        v_b_cap.append(upd_b)\n",
    "        \n",
    "    for i in range(len(w)):\n",
    "        \n",
    "        m_w[i] = beta1*m_w[i] + (1-beta1)*dw[i]\n",
    "        m_b[i] = beta1*m_b[i] + (1-beta1)*db[i]\n",
    "        v_w[i] = beta2*v_w[i] + (1-beta2)*(dw[i])**2\n",
    "        v_b[i] = beta2*v_b[i] + (1-beta2)*(db[i])**2\n",
    "        m_w_cap[i] = (1/(1-beta1**t))*m_w[i]\n",
    "        m_b_cap[i] = (1/(1-beta1**t))*m_b[i]\n",
    "        v_w_cap[i] = (1/(1-beta2**t))*v_w[i]\n",
    "        v_b_cap[i] = (1/(1-beta2**t))*v_b[i]\n",
    "        w[i] = w[i]-(learning_rate/np.sqrt(v_w_cap[i]+epsilon))*(beta1*m_w_cap[i]+(1-beta1)*dw[i]/(1-beta1**t))\n",
    "        b[i] = b[i]-(learning_rate/np.sqrt(v_b_cap[i]+epsilon))*(beta1*m_b_cap[i]+(1-beta1)*db[i]/(1-beta1**t))\n",
    "        \n",
    "    return w,b,m_w,m_b,v_w,v_b\n",
    "        \n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a998a78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ACCURACY\n",
    "def accuracy(y,yout):\n",
    "    '''Function to find the accuracy taking y and ypred as input and returns accracy value.'''\n",
    "    yout=np.argmax(yout,axis=0)\n",
    "    y = np.argmax(y,axis=0)   \n",
    "    acc=np.mean(y==yout)*100\n",
    "    return acc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cd41ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCTION FOR PLOTS\n",
    "def plot_error(j_train, j_val):\n",
    "    plt.plot(list(range(len(j_train))), j_train, 'r', label=\"Train Loss\")\n",
    "    plt.plot(list(range(len(j_val))), j_val, 'lime', label=\"Validation Loss\")\n",
    "    plt.title(\"Training and Validation Loss vs No. of Epochs\", size=16)\n",
    "    plt.xlabel(\"No. of epochs\", size=16)\n",
    "    plt.ylabel(\"Loss\", size=16)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_accuracy(acc_train, acc_val):\n",
    "    plt.plot(list(range(len(acc_train))), acc_train, 'r', label=\"Train Accuracy\")\n",
    "    plt.plot(list(range(len(acc_val))), acc_val, 'lime', label=\"Validation Accuracy\")\n",
    "    plt.title(\"Training and Validation Accuracy vs No. of Epochs\", size=16)\n",
    "    plt.xlabel(\"No. of epochs\", size=16)\n",
    "    plt.ylabel(\"Accuracy\", size=16)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ecd3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    " #PREDICT FUNCTION\n",
    "def predict(x,y,w,b,lambd,activation):\n",
    "    '''This function is to predict the cost and accuracy values of the test data\n",
    "       input :  x(input)\n",
    "                y(output)\n",
    "                w,b(weights and biases)\n",
    "                lamb(regularisation parameter)\n",
    "                loss(loss function)\n",
    "                activation(activation function)'''\n",
    "    \n",
    "    z,a = forward_propagation(x_train,w,b,activation)\n",
    "    acc= accuracy(y,a[-1])\n",
    "    y_pred=np.argmax(a[-1],axis=0)\n",
    "    Y=np.argmax(y,axis=0)\n",
    "            \n",
    "    return y_pred,Y,acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c255eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def neural_network(x_train,y_train,x_val,y_val,learning_rate = 0.1,momentum = 0.9,beta=0.9,beta1=0.9,beta2=0.99,epochs = 20,num_hidden_layers = 1,neurons=64,batch_size=8,epsilon=0.00000001,weight_init='random_initialization',\n",
    "#                   activation='sigmoid_function',loss='cross_entropy_function',optimizer='stochastic_gradient_descent',lambd=0):\n",
    "def neural_network():\n",
    "    \n",
    "    # Default values for hyper-parameters\n",
    "    config_defaults = {\n",
    "        'learning_rate': 0.001,\n",
    "        'activation': 'relu_function',\n",
    "        'weight_init': 'random_initialization',\n",
    "        'optimizer': 'adam',\n",
    "        'batch_size': 8,\n",
    "        'epochs': 20,\n",
    "        'lambd': 0,\n",
    "        'neurons': 64,\n",
    "        'num_hidden_layers': 1,\n",
    "        'momentum':0.9,\n",
    "        'beta':0.9,\n",
    "        'beta1':0.9,\n",
    "        'beta2':0.99,\n",
    "        'loss':'cross_entropy_function'\n",
    "        }\n",
    "\n",
    "\n",
    "    wandb.init(config=config_defaults)\n",
    "    config = wandb.config\n",
    "\n",
    "    learning_rate = config.learning_rate\n",
    "    activation = config.activation\n",
    "    weight_init = config.weight_init\n",
    "    optimizer = config.optimizer\n",
    "    batch_size = config.batch_size\n",
    "    epochs = config.epochs\n",
    "    lambd = config.lambd\n",
    "    neurons = config.neurons\n",
    "    num_hidden_layers = config.num_hidden_layers\n",
    "    momentum=config.momentum\n",
    "    beta = config.beta\n",
    "    beta1=config.beta1\n",
    "    beta2=config.beta2\n",
    "    loss=config.loss\n",
    "    \n",
    "    run_name= f'lr_{learning_rate}_acti_{activation}_w_in{weight_init}_opt_{optimizer}_epoch_{epochs}_num_hid_{num_hidden_layers}_loss_{loss}_batchsize_{batch_size}_neur_{neurons}_lam_{lambd}_momentum_{momentum}_beta_{beta}_beta1_{beta1}_beta2_{beta2}'\n",
    "    print(run_name)\n",
    "#     x_train=x_train_MNIST\n",
    "#     y_train=y_train_MNIST\n",
    "#     x_val=x_val_MNIST\n",
    "#     y_val=y_val_MNIST\n",
    "    epsilon=0.00000001\n",
    "    layer=[]\n",
    "    n,m=x_train.shape\n",
    "    J_train=[]\n",
    "    Accuracy_train=[]\n",
    "    J_val=[]\n",
    "    Accuracy_val=[]\n",
    "    acc_train=0\n",
    "    acc_val=0\n",
    "    layer.append(x_train.shape[0])\n",
    "    for i in range(num_hidden_layers):\n",
    "        layer.append(neurons)\n",
    "    layer.append(y_train.shape[0])\n",
    "    print(f'neuron configuration: {layer}')\n",
    "    if weight_init=='random_initialization':\n",
    "        w,b=random_initialization(layer)\n",
    "    elif weight_init=='xavier_initialization':\n",
    "        w,b=xavier_initialization(layer)\n",
    "    update_w=[]\n",
    "    update_b=[]\n",
    "    w_lookahead=[]\n",
    "    b_lookahead=[]\n",
    "    v_t_w=[]\n",
    "    v_t_b=[]\n",
    "    m_w=[]\n",
    "    m_b=[]\n",
    "    v_w=[]\n",
    "    v_b=[]\n",
    "    for k in range(len(w)):\n",
    "        upd_w=np.zeros((w[k].shape))\n",
    "        upd_b=np.zeros((b[k].shape))\n",
    "        update_w.append(upd_w)\n",
    "        w_lookahead.append(upd_w)\n",
    "        update_b.append(upd_b)\n",
    "        b_lookahead.append(upd_b)\n",
    "        v_t_w.append(upd_w)\n",
    "        v_t_b.append(upd_b)\n",
    "        m_w.append(upd_w)\n",
    "        m_b.append(upd_b)\n",
    "        v_w.append(upd_w)\n",
    "        v_b.append(upd_b)\n",
    "        \n",
    "    num_batches = x_train.shape[1]//batch_size\n",
    "    \n",
    "    if optimizer=='vanilla_gradient_descent':\n",
    "        \n",
    "        for j in range(epochs):\n",
    "            z,a = forward_propagation(x_train,w,b,activation)\n",
    "            dz,dw,db=back_propagation(a,y_train,w,b,z,lambd,activation,loss)\n",
    "            w,b=gradient_descent(w,b,dw,db,learning_rate)\n",
    "            if loss=='cross_entropy_function':\n",
    "                \n",
    "                cost_train=cross_entropy_function(y_train,a[-1],w,lambd)\n",
    "                J_train.append(cost_train)\n",
    "                acc_train = accuracy(y_train,a[-1])\n",
    "                Accuracy_train.append(acc_train)\n",
    "                z_val,a_val = forward_propagation(x_val,w,b,activation)\n",
    "                cost_val = cross_entropy_function(y_val,a_val[-1],w,lambd)\n",
    "                J_val.append(cost_val)\n",
    "                acc_val=accuracy(y_val,a_val[-1])\n",
    "                Accuracy_val.append(acc_val)\n",
    "            elif loss=='mean_squared_error_function':\n",
    "                \n",
    "                cost_train=mean_squared_error_function(y_train,a[-1],w,lambd)\n",
    "                J_train.append(cost_train)\n",
    "                acc_train = accuracy(y_train,a[-1])\n",
    "                Accuracy_train.append(acc_train)\n",
    "                z_val,a_val = forward_propagation(x_val,w,b,activation)\n",
    "                cost_val = mean_squared_error_function(y_val,a_val[-1],w,lambd)\n",
    "                J_val.append(cost_val)\n",
    "                acc_val = accuracy(y_val,a_val[-1])\n",
    "                Accuracy_val.append(acc_val)\n",
    "            wandb.log({\"accuracy_train\": acc_train, \"accuracy_validation\": acc_val, \"loss_train\": cost_train, \"cost_validation\": cost_val, 'epochs': j})\n",
    "            \n",
    "            if j%(epochs/10)==0:\n",
    "                print(f' \\n epoch:{j:4d}  Train error:  {J_train[-1]:8.2f}  Train accuracy: {Accuracy_train[-1]:8.2f} Val error: {J_val[-1]:8.2f} Val accuracy: {Accuracy_val[-1]:8.2f}')\n",
    "            #if j%((epochs/10)/5)==0:\n",
    "                #print('==',end='')\n",
    "            \n",
    "    if optimizer=='stochastic_gradient_descent':\n",
    "        \n",
    "        \n",
    "        for j in range(epochs):\n",
    "            for i in range(m):\n",
    "                x_data = x_train[:,i].reshape(x_train.shape[0],1)\n",
    "                y_data = y_train[:,i].reshape(y_train.shape[0],1)\n",
    "                z,a = forward_propagation(x_data,w,b,activation)\n",
    "                dz,dw,db=back_propagation(a,y_data,w,b,z,lambd,activation,loss)\n",
    "                w,b=gradient_descent(w,b,dw,db,learning_rate)\n",
    "            z,a = forward_propagation(x_train,w,b,activation)\n",
    "            cost_train=cross_entropy_function(y_train,a[-1],w,lambd)\n",
    "            J_train.append(cost_train)\n",
    "            acc_train = accuracy(y_train,a[-1])\n",
    "            Accuracy_train.append(acc_train)\n",
    "            z_val,a_val = forward_propagation(x_val,w,b,activation)\n",
    "            cost_val = cross_entropy_function(y_val,a_val[-1],w,lambd)\n",
    "            J_val.append(cost_val)\n",
    "            acc_val = accuracy(y_val,a_val[-1])\n",
    "            wandb.log({\"accuracy_train\": acc_train, \"accuracy_validation\": acc_val, \"loss_train\": cost_train, \"cost_validation\": cost_val, 'epochs': j})\n",
    "            Accuracy_val.append(acc_val)\n",
    "            if j%(epochs/10)==0:\n",
    "                print(f' \\n epoch:{j:4d}  Train error:  {J_train[-1]:8.2f}  Train accuracy: {Accuracy_train[-1]:8.2f} Val error: {J_val[-1]:8.2f} Val accuracy: {Accuracy_val[-1]:8.2f}')\n",
    "            #if j%((epochs/10)/5)==0:\n",
    "                #print('==',end='')\n",
    "            \n",
    "    if optimizer=='momentum_gradient_descent':\n",
    "            \n",
    "        for j in range(epochs):\n",
    "            \n",
    "            for i in range(num_batches):\n",
    "                x_mb = x_train[:,i*batch_size:(i+1)*batch_size].reshape(x_train.shape[0],batch_size)\n",
    "                y_mb = y_train[:,i*batch_size:(i+1)*batch_size].reshape(y_train.shape[0],batch_size)\n",
    "                z,a = forward_propagation(x_mb,w,b,activation)\n",
    "                dz,dw,db=back_propagation(a,y_mb,w,b,z,lambd,activation,loss)\n",
    "                w,b,update_w,update_b = momentum_gradient_descent(w,b,dw,db,learning_rate,momentum,update_w,update_b)\n",
    "            if x_train.shape[1] % batch_size !=0:\n",
    "                x_last = x_train[:,num_batches*batch_size:]\n",
    "                y_last = y_train[:,num_batches*batch_size:]\n",
    "                z,a = forward_propagation(x_last,w,b,activation)\n",
    "                dz,dw,db=back_propagation(a,y_last,w,b,z,lambd,activation,loss)\n",
    "                w,b,update_w,update_b = momentum_gradient_descent(w,b,dw,db,learning_rate,momentum,update_w,update_b)\n",
    "            z,a = forward_propagation(x_train,w,b,activation)\n",
    "            if loss=='cross_entropy_function':\n",
    "                \n",
    "                cost_train=cross_entropy_function(y_train,a[-1],w,lambd)\n",
    "                J_train.append(cost_train)\n",
    "                acc_train = accuracy(y_train,a[-1])\n",
    "                Accuracy_train.append(acc_train)\n",
    "                z_val,a_val = forward_propagation(x_val,w,b,activation)\n",
    "                cost_val = cross_entropy_function(y_val,a_val[-1],w,lambd)\n",
    "                J_val.append(cost_val)\n",
    "                acc_val=accuracy(y_val,a_val[-1])\n",
    "                Accuracy_val.append(acc_val)\n",
    "            elif loss=='mean_squared_error_function':\n",
    "                \n",
    "                cost_train=mean_squared_error_function(y_train,a[-1],w,lambd)\n",
    "                J_train.append(cost_train)\n",
    "                acc_train = accuracy(y_train,a[-1])\n",
    "                Accuracy_train.append(acc_train)\n",
    "                z_val,a_val = forward_propagation(x_val,w,b,activation)\n",
    "                cost_val = mean_squared_error_function(y_val,a_val[-1],w,lambd)\n",
    "                J_val.append(cost_val)\n",
    "                acc_val = accuracy(y_val,a_val[-1])\n",
    "                Accuracy_val.append(acc_val)\n",
    "            wandb.log({\"accuracy_train\": acc_train, \"accuracy_validation\": acc_val, \"loss_train\": cost_train, \"cost_validation\": cost_val, 'epochs': j})\n",
    "            \n",
    "            if j%(epochs/10)==0:\n",
    "                print(f' \\n epoch:{j:4d}  Train error:  {J_train[-1]:8.2f}  Train accuracy: {Accuracy_train[-1]:8.2f} Val error: {J_val[-1]:8.2f} Val accuracy: {Accuracy_val[-1]:8.2f}')\n",
    "            #if j%((epochs/10)/5)==0:\n",
    "                #print('==',end='')\n",
    "                \n",
    "                \n",
    "    if optimizer == 'nesterov_accelarated_gradient_descent':\n",
    "        \n",
    "        for j in range(epochs):\n",
    "            \n",
    "            for i in range(num_batches):\n",
    "                x_mb = x_train[:,i*batch_size:(i+1)*batch_size].reshape(x_train.shape[0],batch_size)\n",
    "                y_mb = y_train[:,i*batch_size:(i+1)*batch_size].reshape(y_train.shape[0],batch_size)\n",
    "                for i in range(len(w)):\n",
    "                    w_lookahead[i] = w[i]-momentum*update_w[i]\n",
    "                    b_lookahead[i] = b[i]-momentum*update_b[i]\n",
    "                z,a = forward_propagation(x_mb,w_lookahead,b_lookahead,activation)\n",
    "                dz,dw,db=back_propagation(a,y_mb,w_lookahead,b_lookahead,z,lambd,activation,loss)\n",
    "                w,b,update_w,update_b = momentum_gradient_descent(w,b,dw,db,learning_rate,momentum,update_w,update_b)\n",
    "            if x_train.shape[1] % batch_size !=0:\n",
    "                x_last = x_train[:,num_batches*batch_size:]\n",
    "                y_last = y_train[:,num_batches*batch_size:]\n",
    "                z,a = forward_propagation(x_last,w_lookahead,b_lookahead,activation)\n",
    "                dz,dw,db=back_propagation(a,y_last,w_lookahead,b_lookahead,z,lambd,activation,loss)\n",
    "                w,b,update_w,update_b = momentum_gradient_descent(w,b,dw,db,learning_rate,momentum,update_w,update_b)\n",
    "                \n",
    "            z,a = forward_propagation(x_train,w,b,activation)\n",
    "            if loss=='cross_entropy_function':\n",
    "                \n",
    "                cost_train=cross_entropy_function(y_train,a[-1],w,lambd)\n",
    "                J_train.append(cost_train)\n",
    "                acc_train = accuracy(y_train,a[-1])\n",
    "                Accuracy_train.append(acc_train)\n",
    "                z_val,a_val = forward_propagation(x_val,w,b,activation)\n",
    "                cost_val = cross_entropy_function(y_val,a_val[-1],w,lambd)\n",
    "                J_val.append(cost_val)\n",
    "                acc_val=accuracy(y_val,a_val[-1])\n",
    "                Accuracy_val.append(acc_val)\n",
    "            elif loss=='mean_squared_error_function':\n",
    "                \n",
    "                cost_train=mean_squared_error_function(y_train,a[-1],w,lambd)\n",
    "                J_train.append(cost_train)\n",
    "                acc_train = accuracy(y_train,a[-1])\n",
    "                Accuracy_train.append(acc_train)\n",
    "                z_val,a_val = forward_propagation(x_val,w,b,activation)\n",
    "                cost_val = mean_squared_error_function(y_val,a_val[-1],w,lambd)\n",
    "                J_val.append(cost_val)\n",
    "                acc_val = accuracy(y_val,a_val[-1])\n",
    "                Accuracy_val.append(acc_val)\n",
    "            wandb.log({\"accuracy_train\": acc_train, \"accuracy_validation\": acc_val, \"loss_train\": cost_train, \"cost_validation\": cost_val, 'epochs': j})\n",
    "            \n",
    "            if j%(epochs/10)==0:\n",
    "                print(f' \\n epoch:{j:4d}  Train error:  {J_train[-1]:8.2f}  Train accuracy: {Accuracy_train[-1]:8.2f} Val error: {J_val[-1]:8.2f} Val accuracy: {Accuracy_val[-1]:8.2f}')\n",
    "            #if j%((epochs/10)/5)==0:\n",
    "                #print('==',end='')\n",
    "                \n",
    "                \n",
    "    if optimizer == 'rms_prop':\n",
    "            \n",
    "        for j in range(epochs):\n",
    "            \n",
    "            for i in range(num_batches):\n",
    "                x_mb = x_train[:,i*batch_size:(i+1)*batch_size].reshape(x_train.shape[0],batch_size)\n",
    "                y_mb = y_train[:,i*batch_size:(i+1)*batch_size].reshape(y_train.shape[0],batch_size)\n",
    "                z,a = forward_propagation(x_mb,w,b,activation)\n",
    "                dz,dw,db=back_propagation(a,y_mb,w,b,z,lambd,activation,loss)\n",
    "                w,b,v_t_w,v_t_b = rms_prop(w,b,dw,db,learning_rate,beta,epsilon,v_t_w,v_t_b)\n",
    "            if x_train.shape[1] % batch_size !=0:\n",
    "                x_last = x_train[:,num_batches*batch_size:]\n",
    "                y_last = y_train[:,num_batches*batch_size:]\n",
    "                z,a = forward_propagation(x_last,w,b,activation)\n",
    "                dz,dw,db=back_propagation(a,y_last,w,b,z,lambd,activation,loss)\n",
    "                w,b,v_t_w,v_t_b = rms_prop(w,b,dw,db,learning_rate,beta,epsilon,v_t_w,v_t_b)\n",
    "            z,a = forward_propagation(x_train,w,b,activation)\n",
    "            if loss=='cross_entropy_function':\n",
    "                \n",
    "                cost_train=cross_entropy_function(y_train,a[-1],w,lambd)\n",
    "                J_train.append(cost_train)\n",
    "                acc_train = accuracy(y_train,a[-1])\n",
    "                Accuracy_train.append(acc_train)\n",
    "                z_val,a_val = forward_propagation(x_val,w,b,activation)\n",
    "                cost_val = cross_entropy_function(y_val,a_val[-1],w,lambd)\n",
    "                J_val.append(cost_val)\n",
    "                acc_val=accuracy(y_val,a_val[-1])\n",
    "                Accuracy_val.append(acc_val)\n",
    "            elif loss=='mean_squared_error_function':\n",
    "                \n",
    "                cost_train=mean_squared_error_function(y_train,a[-1],w,lambd)\n",
    "                J_train.append(cost_train)\n",
    "                acc_train = accuracy(y_train,a[-1])\n",
    "                Accuracy_train.append(acc_train)\n",
    "                z_val,a_val = forward_propagation(x_val,w,b,activation)\n",
    "                cost_val = mean_squared_error_function(y_val,a_val[-1],w,lambd)\n",
    "                J_val.append(cost_val)\n",
    "                acc_val = accuracy(y_val,a_val[-1])\n",
    "                Accuracy_val.append(acc_val)\n",
    "            wandb.log({\"accuracy_train\": acc_train, \"accuracy_validation\": acc_val, \"loss_train\": cost_train, \"cost_validation\": cost_val, 'epochs': j})\n",
    "            \n",
    "            if j%(epochs/10)==0:\n",
    "                print(f' \\n epoch:{j:4d}  Train error:  {J_train[-1]:8.2f}  Train accuracy: {Accuracy_train[-1]:8.2f} Val error: {J_val[-1]:8.2f} Val accuracy: {Accuracy_val[-1]:8.2f}')\n",
    "            #if j%((epochs/10)/5)==0:\n",
    "                #print('==',end='')\n",
    "                \n",
    "                \n",
    "                \n",
    "    if optimizer == 'adam':\n",
    "        for  j in range(epochs):\n",
    "            \n",
    "            for i in range(num_batches):\n",
    "                x_mb = x_train[:,i*batch_size:(i+1)*batch_size].reshape(x_train.shape[0],batch_size)\n",
    "                y_mb = y_train[:,i*batch_size:(i+1)*batch_size].reshape(y_train.shape[0],batch_size)\n",
    "                z,a = forward_propagation(x_mb,w,b,activation)\n",
    "                dz,dw,db=back_propagation(a,y_mb,w,b,z,lambd,activation,loss)\n",
    "                w,b,m_w,m_b,v_w,v_b = adam(w,b,dw,db,learning_rate,beta1,beta2,epsilon,m_w,m_b,v_w,v_b,j+1)\n",
    "            if x_train.shape[1] % batch_size !=0:\n",
    "                x_last = x_train[:,num_batches*batch_size:]\n",
    "                y_last = y_train[:,num_batches*batch_size:]\n",
    "                z,a = forward_propagation(x_last,w,b,activation)\n",
    "                dz,dw,db=back_propagation(a,y_last,w,b,z,lambd,activation,loss)\n",
    "                w,b,m_w,m_b,v_w,v_b = adam(w,b,dw,db,learning_rate,beta1,beta2,epsilon,m_w,m_b,v_w,v_b,j+1)\n",
    "            z,a=forward_propagation(x_train,w,b,activation)\n",
    "            if loss=='cross_entropy_function':\n",
    "                \n",
    "                cost_train=cross_entropy_function(y_train,a[-1],w,lambd)\n",
    "                J_train.append(cost_train)\n",
    "                acc_train = accuracy(y_train,a[-1])\n",
    "                Accuracy_train.append(acc_train)\n",
    "                z_val,a_val = forward_propagation(x_val,w,b,activation)\n",
    "                cost_val = cross_entropy_function(y_val,a_val[-1],w,lambd)\n",
    "                J_val.append(cost_val)\n",
    "                acc_val=accuracy(y_val,a_val[-1])\n",
    "                Accuracy_val.append(acc_val)\n",
    "            elif loss=='mean_squared_error_function':\n",
    "                \n",
    "                cost_train=mean_squared_error_function(y_train,a[-1],w,lambd)\n",
    "                J_train.append(cost_train)\n",
    "                acc_train = accuracy(y_train,a[-1])\n",
    "                Accuracy_train.append(acc_train)\n",
    "                z_val,a_val = forward_propagation(x_val,w,b,activation)\n",
    "                cost_val = mean_squared_error_function(y_val,a_val[-1],w,lambd)\n",
    "                J_val.append(cost_val)\n",
    "                acc_val = accuracy(y_val,a_val[-1])\n",
    "                Accuracy_val.append(acc_val)\n",
    "            wandb.log({\"accuracy_train\": acc_train, \"accuracy_validation\": acc_val, \"loss_train\": cost_train, \"cost_validation\": cost_val, 'epochs': j})\n",
    "            \n",
    "            if j%(epochs/10)==0:\n",
    "                print(f' \\n epoch:{j:4d}  Train error:  {J_train[-1]:8.2f}  Train accuracy: {Accuracy_train[-1]:8.2f} Val error: {J_val[-1]:8.2f} Val accuracy: {Accuracy_val[-1]:8.2f}')\n",
    "            #if j%((epochs/10)/5)==0:\n",
    "                #print('==',end='')\n",
    "                \n",
    "                \n",
    "    if optimizer =='nadam':\n",
    "        for  j in range(epochs):\n",
    "            \n",
    "            for i in range(num_batches):\n",
    "                x_mb = x_train[:,i*batch_size:(i+1)*batch_size].reshape(x_train.shape[0],batch_size)\n",
    "                y_mb = y_train[:,i*batch_size:(i+1)*batch_size].reshape(y_train.shape[0],batch_size)\n",
    "                z,a = forward_propagation(x_mb,w,b,activation)\n",
    "                dz,dw,db=back_propagation(a,y_mb,w,b,z,lambd,activation,loss)\n",
    "                w,b,m_w,m_b,v_w,v_b = nadam(w,b,dw,db,learning_rate,beta1,beta2,epsilon,m_w,m_b,v_w,v_b,j+1)\n",
    "            if x_train.shape[1] % batch_size !=0:\n",
    "                x_last = x_train[:,num_batches*batch_size:]\n",
    "                y_last = y_train[:,num_batches*batch_size:]\n",
    "                z,a = forward_propagation(x_last,w,b,activation)\n",
    "                dz,dw,db=back_propagation(a,y_last,w,b,z,lambd,activation,loss)\n",
    "                w,b,m_w,m_b,v_w,v_b = nadam(w,b,dw,db,learning_rate,beta1,beta2,epsilon,m_w,m_b,v_w,v_b,j+1)\n",
    "            z,a=forward_propagation(x_train,w,b,activation)\n",
    "            \n",
    "            if loss=='cross_entropy_function':\n",
    "                \n",
    "                cost_train=cross_entropy_function(y_train,a[-1],w,lambd)\n",
    "                J_train.append(cost_train)\n",
    "                acc_train = accuracy(y_train,a[-1])\n",
    "                Accuracy_train.append(acc_train)\n",
    "                z_val,a_val = forward_propagation(x_val,w,b,activation)\n",
    "                cost_val = cross_entropy_function(y_val,a_val[-1],w,lambd)\n",
    "                J_val.append(cost_val)\n",
    "                acc_val=accuracy(y_val,a_val[-1])\n",
    "                Accuracy_val.append(acc_val)\n",
    "            elif loss=='mean_squared_error_function':\n",
    "                \n",
    "                cost_train=mean_squared_error_function(y_train,a[-1],w,lambd)\n",
    "                J_train.append(cost_train)\n",
    "                acc_train = accuracy(y_train,a[-1])\n",
    "                Accuracy_train.append(acc_train)\n",
    "                z_val,a_val = forward_propagation(x_val,w,b,activation)\n",
    "                cost_val = mean_squared_error_function(y_val,a_val[-1],w,lambd)\n",
    "                J_val.append(cost_val)\n",
    "                acc_val = accuracy(y_val,a_val[-1])\n",
    "                Accuracy_val.append(acc_val)\n",
    "            wandb.log({\"accuracy_train\": acc_train, \"accuracy_validation\": acc_val, \"loss_train\": cost_train, \"cost_validation\": cost_val, 'epochs': j})\n",
    "            \n",
    "            if j%(epochs/10)==0:\n",
    "                print(f' \\n epoch:{j:4d}  Train error:  {J_train[-1]:8.2f}  Train accuracy: {Accuracy_train[-1]:8.2f} Val error: {J_val[-1]:8.2f} Val accuracy: {Accuracy_val[-1]:8.2f}')\n",
    "            #if j%((epochs/10)/5)==0:\n",
    "                #print('==',end='')\n",
    "        \n",
    "            \n",
    "            \n",
    "    plot_error(J_train,J_val)\n",
    "    plot_accuracy(Accuracy_train,Accuracy_val)\n",
    "    \n",
    "    wandb.run.name = run_name\n",
    "    wandb.run.save()\n",
    "    wandb.run.finish()\n",
    "    return w,b,J_train,J_val\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29922750",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''INITIAL RANDOM SWEEP'''\n",
    "sweep_config = {\n",
    "  \"name\": \"Loss and Accuracy\",\n",
    "  \"metric\": {\n",
    "      \"name\":\"accuracy_validation\",\n",
    "      \"goal\": \"maximize\"\n",
    "  },\n",
    "  \"method\": \"random\",\n",
    "  \"parameters\": {\n",
    "        \"learning_rate\": {\n",
    "            \"values\": [0.1,0.01,0.001, 0.0001,0.00001]\n",
    "        },\n",
    "        \"activation\": {\n",
    "            \"values\": [\"sigmoid_function\", \"relu_function\", \"tanh_function\",\"identity_function\"]\n",
    "        },\n",
    "        \"weight_init\": {\n",
    "            \"values\": [\"xavier_initialization\", \"random_initialization\"]\n",
    "        },\n",
    "        \"optimizer\": {\n",
    "            \"values\": [\"stochastic_gradient_descent\", \"momentum_gradient_descent\", \"nesterov_accelarated_gradient_descent\", \"adam\", \"nadam\", \"rms_prop\"]\n",
    "        },\n",
    "        \"batch_size\": {\n",
    "            \"values\": [16,32,64]\n",
    "        },\n",
    "        \"epochs\": {\n",
    "            \"values\": [10,20,40]\n",
    "        },\n",
    "        \"lambd\": {\n",
    "            \"values\": [0, 0.0005, 0.01]\n",
    "        },\n",
    "        \"neurons\": {\n",
    "            \"values\": [16,32,64]\n",
    "        },\n",
    "        \"num_hidden_layers\": {\n",
    "            \"values\": [1,2]\n",
    "        },\n",
    "        \"momentum\": {\n",
    "            \"values\": [0.8,0.9]\n",
    "        },\n",
    "        \"beta\": {\n",
    "            \"values\": [0.9]\n",
    "        },\n",
    "        \"beta1\": {\n",
    "            \"values\": [0.9]\n",
    "        },\n",
    "        \"beta2\": {\n",
    "            \"values\": [0.99]\n",
    "        },\n",
    "        \"loss\": {\n",
    "            \"values\": ['cross_entropy_function']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"CS6910_DL_ASS1\")\n",
    "wandb.agent(sweep_id, neural_network, count=150)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd83bc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' BAYES SWEEP'''\n",
    "sweep_config = {\n",
    "  \"name\": \"Loss and Accuracy\",\n",
    "  \"metric\": {\n",
    "      \"name\":\"accuracy_validation\",\n",
    "      \"goal\": \"maximize\"\n",
    "  },\n",
    "  \"method\": \"bayes\",\n",
    "  \"parameters\": {\n",
    "        \"learning_rate\": {\n",
    "            \"values\": [0.01,0.001]\n",
    "        },\n",
    "        \"activation\": {\n",
    "            \"values\": [ \"relu_function\"]\n",
    "        },\n",
    "        \"weight_init\": {\n",
    "            \"values\": [ \"random_initialization\"]\n",
    "        },\n",
    "        \"optimizer\": {\n",
    "            \"values\": [\"nadam\", \"nesterov_accelarated_gradient_descent\", \"adam\", \"rms_prop\"]\n",
    "        },\n",
    "        \"batch_size\": {\n",
    "            \"values\": [4,8,16]\n",
    "        },\n",
    "        \"epochs\": {\n",
    "            \"values\": [10,20]\n",
    "        },\n",
    "        \"lambd\": {\n",
    "            \"values\": [0, 0.001]\n",
    "        },\n",
    "        \"neurons\": {\n",
    "            \"values\": [32,64,128]\n",
    "        },\n",
    "        \"num_hidden_layers\": {\n",
    "            \"values\": [1,2]\n",
    "        },\n",
    "        \"momentum\": {\n",
    "            \"values\": [0.9]\n",
    "        },\n",
    "        \"beta\": {\n",
    "            \"values\": [0.9]\n",
    "        },\n",
    "        \"beta1\": {\n",
    "            \"values\": [0.9]\n",
    "        },\n",
    "        \"beta2\": {\n",
    "            \"values\": [0.99]\n",
    "        },\n",
    "        \"loss\": {\n",
    "            \"values\": ['cross_entropy_function']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"CS6910_DL_ASS1\")\n",
    "wandb.agent(sweep_id, neural_network, count=150)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f929300d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''GRID SWEEP'''\n",
    "sweep_config = {\n",
    "  \"name\": \"Loss and Accuracy-Highest validation accuracy\",\n",
    "  \"metric\": {\n",
    "      \"name\":\"accuracy_validation\",\n",
    "      \"goal\": \"maximize\"\n",
    "  },\n",
    "  \"method\": \"grid\",\n",
    "  \"parameters\": {\n",
    "        \"learning_rate\": {\n",
    "            \"values\": [0.001,0.0001]\n",
    "        },\n",
    "        \"activation\": {\n",
    "            \"values\": [\"relu_function\"]\n",
    "        },\n",
    "        \"weight_init\": {\n",
    "            \"values\": [\"random_initialization\"]\n",
    "        },\n",
    "        \"optimizer\": {\n",
    "            \"values\": [\"adam\",\"nadam\"]\n",
    "        },\n",
    "        \"batch_size\": {\n",
    "            \"values\": [2,4,8]\n",
    "        },\n",
    "        \"epochs\": {\n",
    "            \"values\": [10,20]\n",
    "        },\n",
    "        \"lambd\": {\n",
    "            \"values\": [0,0.001]\n",
    "        },\n",
    "        \"neurons\": {\n",
    "            \"values\": [64]\n",
    "        },\n",
    "        \"num_hidden_layers\": {\n",
    "            \"values\": [1]\n",
    "        },\n",
    "        \"momentum\": {\n",
    "            \"values\": [0.9]\n",
    "        },\n",
    "        \"beta\": {\n",
    "            \"values\": [0.9]\n",
    "        },\n",
    "        \"beta1\": {\n",
    "            \"values\": [0.9]\n",
    "        },\n",
    "        \"beta2\": {\n",
    "            \"values\": [0.99]\n",
    "        },\n",
    "        \"loss\": {\n",
    "            \"values\": ['cross_entropy_function']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"CS6910_DL_ASS1\")\n",
    "wandb.agent(sweep_id, neural_network)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cefdd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "  \"name\": \"Loss and Accuracy-Highest validation accuracy\",\n",
    "  \"metric\": {\n",
    "      \"name\":\"accuracy_validation\",\n",
    "      \"goal\": \"maximize\"\n",
    "  },\n",
    "  \"method\": \"grid\",\n",
    "  \"parameters\": {\n",
    "        \"learning_rate\": {\n",
    "            \"values\": [0.01,0.001]\n",
    "        },\n",
    "        \"activation\": {\n",
    "            \"values\": [\"relu_function\"]\n",
    "        },\n",
    "        \"weight_init\": {\n",
    "            \"values\": [\"random_initialization\"]\n",
    "        },\n",
    "        \"optimizer\": {\n",
    "            \"values\": [\"adam\",\"nadam\"]\n",
    "        },\n",
    "        \"batch_size\": {\n",
    "            \"values\": [16]\n",
    "        },\n",
    "        \"epochs\": {\n",
    "            \"values\": [10,20]\n",
    "        },\n",
    "        \"lambd\": {\n",
    "            \"values\": [0]\n",
    "        },\n",
    "        \"neurons\": {\n",
    "            \"values\": [64,128]\n",
    "        },\n",
    "        \"num_hidden_layers\": {\n",
    "            \"values\": [1]\n",
    "        },\n",
    "        \"momentum\": {\n",
    "            \"values\": [0.9]\n",
    "        },\n",
    "        \"beta\": {\n",
    "            \"values\": [0.9]\n",
    "        },\n",
    "        \"beta1\": {\n",
    "            \"values\": [0.9]\n",
    "        },\n",
    "        \"beta2\": {\n",
    "            \"values\": [0.99]\n",
    "        },\n",
    "        \"loss\": {\n",
    "            \"values\": ['cross_entropy_function']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"CS6910_DL_ASS1\")\n",
    "wandb.agent(sweep_id, neural_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1a7687",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "  \"name\": \"Loss and Accuracy-Highest validation accuracy\",\n",
    "  \"metric\": {\n",
    "      \"name\":\"accuracy_validation\",\n",
    "      \"goal\": \"maximize\"\n",
    "  },\n",
    "  \"method\": \"grid\",\n",
    "  \"parameters\": {\n",
    "        \"learning_rate\": {\n",
    "            \"values\": [0.001]\n",
    "        },\n",
    "        \"activation\": {\n",
    "            \"values\": [\"relu_function\"]\n",
    "        },\n",
    "        \"weight_init\": {\n",
    "            \"values\": [\"random_initialization\"]\n",
    "        },\n",
    "        \"optimizer\": {\n",
    "            \"values\": [\"nadam\",\"adam\"]\n",
    "        },\n",
    "        \"batch_size\": {\n",
    "            \"values\": [8]\n",
    "        },\n",
    "        \"epochs\": {\n",
    "            \"values\": [20,40]\n",
    "        },\n",
    "        \"lambd\": {\n",
    "            \"values\": [0]\n",
    "        },\n",
    "        \"neurons\": {\n",
    "            \"values\": [64]\n",
    "        },\n",
    "        \"num_hidden_layers\": {\n",
    "            \"values\": [1]\n",
    "        },\n",
    "        \"momentum\": {\n",
    "            \"values\": [0.9]\n",
    "        },\n",
    "        \"beta\": {\n",
    "            \"values\": [0.9]\n",
    "        },\n",
    "        \"beta1\": {\n",
    "            \"values\": [0.9]\n",
    "        },\n",
    "        \"beta2\": {\n",
    "            \"values\": [0.99]\n",
    "        },\n",
    "        \"loss\": {\n",
    "            \"values\": ['cross_entropy_function']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"CS6910_DL_ASS1\")\n",
    "wandb.agent(sweep_id, neural_network)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4d9a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "  \"name\": \"Loss and Accuracy-Highest validation accuracy\",\n",
    "  \"metric\": {\n",
    "      \"name\":\"accuracy_validation\",\n",
    "      \"goal\": \"maximize\"\n",
    "  },\n",
    "  \"method\": \"grid\",\n",
    "  \"parameters\": {\n",
    "        \"learning_rate\": {\n",
    "            \"values\": [0.001]\n",
    "        },\n",
    "        \"activation\": {\n",
    "            \"values\": [\"relu_function\"]\n",
    "        },\n",
    "        \"weight_init\": {\n",
    "            \"values\": [\"random_initialization\"]\n",
    "        },\n",
    "        \"optimizer\": {\n",
    "            \"values\": [\"nadam\",\"adam\"]\n",
    "        },\n",
    "        \"batch_size\": {\n",
    "            \"values\": [8]\n",
    "        },\n",
    "        \"epochs\": {\n",
    "            \"values\": [10,15,20]\n",
    "        },\n",
    "        \"lambd\": {\n",
    "            \"values\": [0]\n",
    "        },\n",
    "        \"neurons\": {\n",
    "            \"values\": [64]\n",
    "        },\n",
    "        \"num_hidden_layers\": {\n",
    "            \"values\": [1]\n",
    "        },\n",
    "        \"momentum\": {\n",
    "            \"values\": [0.9]\n",
    "        },\n",
    "        \"beta\": {\n",
    "            \"values\": [0.9]\n",
    "        },\n",
    "        \"beta1\": {\n",
    "            \"values\": [0.9]\n",
    "        },\n",
    "        \"beta2\": {\n",
    "            \"values\": [0.99]\n",
    "        },\n",
    "        \"loss\": {\n",
    "            \"values\": ['cross_entropy_function','mean_squared_error_function']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"CS6910_DL_ASS1\")\n",
    "wandb.agent(sweep_id, neural_network)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03042385",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''HIGHEST ACCURACY'''\n",
    "sweep_config = {\n",
    "  \"name\": \"Loss and Accuracy-Highest validation accuracy\",\n",
    "  \"metric\": {\n",
    "      \"name\":\"accuracy_validation\",\n",
    "      \"goal\": \"maximize\"\n",
    "  },\n",
    "  \"method\": \"grid\",\n",
    "  \"parameters\": {\n",
    "        \"learning_rate\": {\n",
    "            \"values\": [0.001]\n",
    "        },\n",
    "        \"activation\": {\n",
    "            \"values\": [\"relu_function\"]\n",
    "        },\n",
    "        \"weight_init\": {\n",
    "            \"values\": [\"random_initialization\"]\n",
    "        },\n",
    "        \"optimizer\": {\n",
    "            \"values\": [\"adam\"]\n",
    "        },\n",
    "        \"batch_size\": {\n",
    "            \"values\": [8]\n",
    "        },\n",
    "        \"epochs\": {\n",
    "            \"values\": [20]\n",
    "        },\n",
    "        \"lambd\": {\n",
    "            \"values\": [0]\n",
    "        },\n",
    "        \"neurons\": {\n",
    "            \"values\": [64]\n",
    "        },\n",
    "        \"num_hidden_layers\": {\n",
    "            \"values\": [1]\n",
    "        },\n",
    "        \"momentum\": {\n",
    "            \"values\": [0.9]\n",
    "        },\n",
    "        \"beta\": {\n",
    "            \"values\": [0.9]\n",
    "        },\n",
    "        \"beta1\": {\n",
    "            \"values\": [0.9]\n",
    "        },\n",
    "        \"beta2\": {\n",
    "            \"values\": [0.99]\n",
    "        },\n",
    "        \"loss\": {\n",
    "            \"values\": ['cross_entropy_function']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"CS6910_DL_ASS1\")\n",
    "wandb.agent(sweep_id, neural_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb53a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''HIGHEST ACCURACY\n",
    "EARNING RATE:0.001\n",
    "EPOCHS:20\n",
    "OPTIMIZER:Adam\n",
    "REGULARIZATION PARAMETER:0.0001\n",
    "ACTIVATION FUNCTION:RELu function\n",
    "BETA1:0.9\n",
    "BETA2:0.99\n",
    "INITIALIZATION:RANDOM\n",
    "LOSS FUNCTION:CROSS-ENTROPY\n",
    "BATCH SIZE:8\n",
    "NEURONS:64\n",
    "HIDDEN LAYERS:1'''\n",
    "w_max,b_max,J_train,J_val=neural_network(x_train,y_train,x_val,y_val,learning_rate=0.001,epochs=20,activation='relu_function',optimizer='adam',loss='cross_entropy_function',\n",
    "                                        batch_size=8,num_hidden_layers=1,neurons=64,lambd=0,epsilon=0.00000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af83ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''ACCURACY OF VALIDATION DATA FOR THE HIGHEST ACCURACY CASE'''\n",
    "z,a=forward_propagation(x_val,w_max,b_max,activation='relu_function')\n",
    "y_pred=a[-1]\n",
    "y_pred=np.argmax(y_pred,axis=0)\n",
    "Y=np.argmax(y_val,axis=0)\n",
    "acc_val=np.mean(Y_pred==Y)*100\n",
    "print(f'Highest accuracy of validation data is :{acc_val}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f514fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''ACCURACY OF TEST DATA FOR THE HIGHEST ACCURACY CASE'''\n",
    "z_test,a_test=forward_propagation(x_test,w_max,b_max,activation='relu_function')\n",
    "y_pred_test=a_test[-1]\n",
    "y_pred_test=np.argmax(y_pred_test,axis=0)\n",
    "Y_test=np.argmax(y_test,axis=0)\n",
    "acc_test=np.mean(y_pred_test==Y_test)*100\n",
    "print(f'Highest accuracy of test data is :{acc_test}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c961fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''VALIDATION DATA'''\n",
    "\n",
    "cm = confusion_matrix(Y, Y_pred)\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(cm, annot=True, cmap=\"binary\", fmt='g', ax=ax,xticklabels=['T-shirt','Trouser','Pullover','Skirt',\n",
    "    'Overcoat','Sandal','Shirt','Sneakers','Bag','Ankle boot'],yticklabels=['T-shirt','Trouser','Pullover','Skirt',\n",
    "                                                        'Overcoat','Sandal','Shirt','Sneakers','Bag','Ankle boot'])\n",
    "ax.set_xlabel(\"Predicted labels\")\n",
    "ax.set_ylabel(\"True labels\")\n",
    "ax.set_title(\"Confusion Matrix\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607dcb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''TEST DATA'''\n",
    "\n",
    "cm = confusion_matrix(Y_test, y_pred_test)\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(cm, annot=True, cmap=\"binary\", fmt='g', ax=ax,xticklabels=['T-shirt','Trouser','Pullover','Skirt',\n",
    "    'Overcoat','Sandal','Shirt','Sneakers','Bag','Ankle boot'],yticklabels=['T-shirt','Trouser','Pullover','Skirt',\n",
    "                                                        'Overcoat','Sandal','Shirt','Sneakers','Bag','Ankle boot'])\n",
    "ax.set_xlabel(\"Predicted labels\")\n",
    "ax.set_ylabel(\"True labels\")\n",
    "ax.set_title(\"Confusion Matrix\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803493fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''CONFUSION MATRIX FOR VALIDATION DATA'''\n",
    "wandb.init(project=\"CS6910_DL_ASS1\", name='CONFUSION MATRIX')\n",
    "\n",
    "wandb.log({\"conf_mat\" : wandb.plot.confusion_matrix(probs=None,\n",
    "                        y_true=Y, preds=Y_pred,class_names=['T-shirt','Trouser','Pullover','Skirt',\n",
    "                                                        'Overcoat','Sandal','Shirt','Sneakers','Bag','Ankle boot'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026d1aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t=copy.deepcopy(x_train)\n",
    "X_v=copy.deepcopy(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53d273c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''DATA AUGMENTATION BY ADDING NOISE TO TRAIN DATA'''\n",
    "n,m=x_train.shape\n",
    "p,q=x_val.shape\n",
    "mean=0\n",
    "variance=0.01\n",
    "std=np.sqrt(variance)\n",
    "k=np.random.normal(loc=mean,scale=std,size=(n,m))\n",
    "e=np.random.normal(loc=mean,scale=std,size=(p,q))\n",
    "x_train=x_train+k\n",
    "x_train=np.clip(x_train,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae25029",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''data augmentation-gaussian error addition'''\n",
    "sweep_config = {\n",
    "  \"name\": \"Loss and Accuracy-Highest validation accuracy\",\n",
    "  \"metric\": {\n",
    "      \"name\":\"accuracy_validation\",\n",
    "      \"goal\": \"maximize\"\n",
    "  },\n",
    "  \"method\": \"grid\",\n",
    "  \"parameters\": {\n",
    "        \"learning_rate\": {\n",
    "            \"values\": [0.01,0.001]\n",
    "        },\n",
    "        \"activation\": {\n",
    "            \"values\": [\"relu_function\"]\n",
    "        },\n",
    "        \"weight_init\": {\n",
    "            \"values\": [\"random_initialization\"]\n",
    "        },\n",
    "        \"optimizer\": {\n",
    "            \"values\": [\"nadam\",\"adam\"]\n",
    "        },\n",
    "        \"batch_size\": {\n",
    "            \"values\": [8,16]\n",
    "        },\n",
    "        \"epochs\": {\n",
    "            \"values\": [10,15,20]\n",
    "        },\n",
    "        \"lambd\": {\n",
    "            \"values\": [0,0.001]\n",
    "        },\n",
    "        \"neurons\": {\n",
    "            \"values\": [32,64]\n",
    "        },\n",
    "        \"num_hidden_layers\": {\n",
    "            \"values\": [1]\n",
    "        },\n",
    "        \"momentum\": {\n",
    "            \"values\": [0.9]\n",
    "        },\n",
    "        \"beta\": {\n",
    "            \"values\": [0.9]\n",
    "        },\n",
    "        \"beta1\": {\n",
    "            \"values\": [0.9]\n",
    "        },\n",
    "        \"beta2\": {\n",
    "            \"values\": [0.99]\n",
    "        },\n",
    "        \"loss\": {\n",
    "            \"values\": ['cross_entropy_function']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"CS6910_DL_ASS1\")\n",
    "wandb.agent(sweep_id, neural_network)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608f068d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''data augmentation-gaussian error addition'''\n",
    "sweep_config = {\n",
    "  \"name\": \"Loss and Accuracy-Highest validation accuracy\",\n",
    "  \"metric\": {\n",
    "      \"name\":\"accuracy_validation\",\n",
    "      \"goal\": \"maximize\"\n",
    "  },\n",
    "  \"method\": \"grid\",\n",
    "  \"parameters\": {\n",
    "        \"learning_rate\": {\n",
    "            \"values\": [0.001]\n",
    "        },\n",
    "        \"activation\": {\n",
    "            \"values\": [\"relu_function\"]\n",
    "        },\n",
    "        \"weight_init\": {\n",
    "            \"values\": [\"random_initialization\"]\n",
    "        },\n",
    "        \"optimizer\": {\n",
    "            \"values\": [\"adam\"]\n",
    "        },\n",
    "        \"batch_size\": {\n",
    "            \"values\": [32]\n",
    "        },\n",
    "        \"epochs\": {\n",
    "            \"values\": [20]\n",
    "        },\n",
    "        \"lambd\": {\n",
    "            \"values\": [0]\n",
    "        },\n",
    "        \"neurons\": {\n",
    "            \"values\": [64]\n",
    "        },\n",
    "        \"num_hidden_layers\": {\n",
    "            \"values\": [1]\n",
    "        },\n",
    "        \"momentum\": {\n",
    "            \"values\": [0.9]\n",
    "        },\n",
    "        \"beta\": {\n",
    "            \"values\": [0.9]\n",
    "        },\n",
    "        \"beta1\": {\n",
    "            \"values\": [0.9]\n",
    "        },\n",
    "        \"beta2\": {\n",
    "            \"values\": [0.99]\n",
    "        },\n",
    "        \"loss\": {\n",
    "            \"values\": ['cross_entropy_function']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"CS6910_DL_ASS1\")\n",
    "wandb.agent(sweep_id, neural_network)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b981360",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''MNIST DATASET'''\n",
    "'''LOADING TRAIN AND TEST DATA SET'''\n",
    "(X_train_MNIST,Y_train_MNIST),(X_test_MNIST,Y_test_MNIST) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f717ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Shape of input train data is : {X_train_MNIST.shape}')\n",
    "print(f'Shape of input test data is : {X_test_MNIST.shape}')\n",
    "print(f'Shape of output train data is : {Y_train_MNIST.shape}')\n",
    "print(f'Shape of output test data is : {Y_test_MNIST.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c8a147",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''CHANGING THE SHAPE OF INPUT DATA'''\n",
    "x_train_MNIST=np.zeros((60000,784))\n",
    "for i in range(X_train_MNIST.shape[0]):\n",
    "    a=X_train_MNIST[i].reshape(1,784)\n",
    "    x_train_MNIST[i]=a\n",
    "print('Train data',x_train_MNIST.shape)\n",
    "x_test_MNIST=np.zeros((10000,784))\n",
    "for i in range(X_test_MNIST.shape[0]):\n",
    "    a=X_test_MNIST[i].reshape(1,784)\n",
    "    x_test_MNIST[i]=a\n",
    "print('Test data',x_test_MNIST.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2325be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''CONVERTING OUTPUT DATA INTO ONE HOT VECTOR FORM'''\n",
    "a = np.max(Y_train_MNIST)+1\n",
    "y_train_MNIST=np.zeros((Y_train_MNIST.shape[0],a))\n",
    "for i in range(Y_train_MNIST.shape[0]):\n",
    "    for j in range(a):\n",
    "        if Y_train_MNIST[i]==j:\n",
    "            y_train_MNIST[i,j]=1\n",
    "print('Train data',y_train_MNIST.shape)\n",
    "y_test_MNIST=np.zeros((Y_test_MNIST.shape[0],a))\n",
    "for i in range(Y_test_MNIST.shape[0]):\n",
    "    for j in range(a):\n",
    "        if Y_test_MNIST[i]==j:\n",
    "            y_test_MNIST[i,j]=1\n",
    "#print(y_test[0,:])\n",
    "print('Test data',y_test_MNIST.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248dc4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''CREATING VALIDATION DATA SET'''\n",
    "'''The input data is split into train and validation data where validation comprises of 10% of the data.'''\n",
    "\n",
    "x_train_MNIST,x_val_MNIST,y_train_MNIST,y_val_MNIST = train_test_split(x_train_MNIST,y_train_MNIST,test_size=0.10,random_state=10)\n",
    "x_train_MNIST=x_train_MNIST.T/255\n",
    "y_train_MNIST=y_train_MNIST.T\n",
    "x_test_MNIST=x_test_MNIST.T/255\n",
    "y_test_MNIST=y_test_MNIST.T\n",
    "x_val_MNIST=x_val_MNIST.T/255\n",
    "y_val_MNIST=y_val_MNIST.T\n",
    "print(f'The shape of train data is:{x_train_MNIST.shape}')\n",
    "print(f'The shape of test data is:{x_test_MNIST.shape}')\n",
    "print(f'The shape of validation data is:{x_val_MNIST.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f840c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''HIGHEST ACCURACY PLOT FOR MNIST DATASET'''\n",
    "sweep_config = {\n",
    "  \"name\": \"Loss and Accuracy-MNIST\",\n",
    "  \"metric\": {\n",
    "      \"name\":\"accuracy_validation\",\n",
    "      \"goal\": \"maximize\"\n",
    "  },\n",
    "  \"method\": \"grid\",\n",
    "  \"parameters\": {\n",
    "        \"learning_rate\": {\n",
    "            \"values\": [0.001]\n",
    "        },\n",
    "        \"activation\": {\n",
    "            \"values\": [\"relu_function\"]\n",
    "        },\n",
    "        \"weight_init\": {\n",
    "            \"values\": [\"random_initialization\"]\n",
    "        },\n",
    "        \"optimizer\": {\n",
    "            \"values\": [\"adam\"]\n",
    "        },\n",
    "        \"batch_size\": {\n",
    "            \"values\": [16]\n",
    "        },\n",
    "        \"epochs\": {\n",
    "            \"values\": [20]\n",
    "        },\n",
    "        \"lambd\": {\n",
    "            \"values\": [0]\n",
    "        },\n",
    "        \"neurons\": {\n",
    "            \"values\": [128]\n",
    "        },\n",
    "        \"num_hidden_layers\": {\n",
    "            \"values\": [1]\n",
    "        },\n",
    "        \"momentum\": {\n",
    "            \"values\": [0.9]\n",
    "        },\n",
    "        \"beta\": {\n",
    "            \"values\": [0.9]\n",
    "        },\n",
    "        \"beta1\": {\n",
    "            \"values\": [0.9]\n",
    "        },\n",
    "        \"beta2\": {\n",
    "            \"values\": [0.99]\n",
    "        },\n",
    "        \"loss\": {\n",
    "            \"values\": ['cross_entropy_function']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"CS6910_DL_ASS1\")\n",
    "wandb.agent(sweep_id, neural_network)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d7f832",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''HIGHEST ACCURACY PLOT FOR MNIST DATASET'''\n",
    "sweep_config = {\n",
    "  \"name\": \"Loss and Accuracy-MNIST\",\n",
    "  \"metric\": {\n",
    "      \"name\":\"accuracy_validation\",\n",
    "      \"goal\": \"maximize\"\n",
    "  },\n",
    "  \"method\": \"grid\",\n",
    "  \"parameters\": {\n",
    "        \"learning_rate\": {\n",
    "            \"values\": [0.001]\n",
    "        },\n",
    "        \"activation\": {\n",
    "            \"values\": [\"relu_function\"]\n",
    "        },\n",
    "        \"weight_init\": {\n",
    "            \"values\": [\"random_initialization\"]\n",
    "        },\n",
    "        \"optimizer\": {\n",
    "            \"values\": [\"adam\",\"nadam\"]\n",
    "        },\n",
    "        \"batch_size\": {\n",
    "            \"values\": [8,16]\n",
    "        },\n",
    "        \"epochs\": {\n",
    "            \"values\": [10,20]\n",
    "        },\n",
    "        \"lambd\": {\n",
    "            \"values\": [0.001]\n",
    "        },\n",
    "        \"neurons\": {\n",
    "            \"values\": [64]\n",
    "        },\n",
    "        \"num_hidden_layers\": {\n",
    "            \"values\": [1]\n",
    "        },\n",
    "        \"momentum\": {\n",
    "            \"values\": [0.9]\n",
    "        },\n",
    "        \"beta\": {\n",
    "            \"values\": [0.9]\n",
    "        },\n",
    "        \"beta1\": {\n",
    "            \"values\": [0.9]\n",
    "        },\n",
    "        \"beta2\": {\n",
    "            \"values\": [0.99]\n",
    "        },\n",
    "        \"loss\": {\n",
    "            \"values\": ['cross_entropy_function']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"CS6910_DL_ASS1\")\n",
    "wandb.agent(sweep_id, neural_network)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7413d99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''HIGHEST ACCURACY PLOT FOR MNIST DATASET'''\n",
    "sweep_config = {\n",
    "  \"name\": \"Loss and Accuracy-MNIST\",\n",
    "  \"metric\": {\n",
    "      \"name\":\"accuracy_validation\",\n",
    "      \"goal\": \"maximize\"\n",
    "  },\n",
    "  \"method\": \"grid\",\n",
    "  \"parameters\": {\n",
    "        \"learning_rate\": {\n",
    "            \"values\": [0.01]\n",
    "        },\n",
    "        \"activation\": {\n",
    "            \"values\": [\"relu_function\"]\n",
    "        },\n",
    "        \"weight_init\": {\n",
    "            \"values\": [\"random_initialization\"]\n",
    "        },\n",
    "        \"optimizer\": {\n",
    "            \"values\": [\"rms_prop\"]\n",
    "        },\n",
    "        \"batch_size\": {\n",
    "            \"values\": [32,64]\n",
    "        },\n",
    "        \"epochs\": {\n",
    "            \"values\": [10]\n",
    "        },\n",
    "        \"lambd\": {\n",
    "            \"values\": [0.001,0.01]\n",
    "        },\n",
    "        \"neurons\": {\n",
    "            \"values\": [32]\n",
    "        },\n",
    "        \"num_hidden_layers\": {\n",
    "            \"values\": [1]\n",
    "        },\n",
    "        \"momentum\": {\n",
    "            \"values\": [0.9]\n",
    "        },\n",
    "        \"beta\": {\n",
    "            \"values\": [0.9]\n",
    "        },\n",
    "        \"beta1\": {\n",
    "            \"values\": [0.9]\n",
    "        },\n",
    "        \"beta2\": {\n",
    "            \"values\": [0.99]\n",
    "        },\n",
    "        \"loss\": {\n",
    "            \"values\": ['cross_entropy_function']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"CS6910_DL_ASS1\")\n",
    "wandb.agent(sweep_id, neural_network)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
