{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "32eebf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import copy\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "43fd3c13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0f1d0668",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''LOADING TRAIN AND TEST DATA SET'''\n",
    "(X_train,Y_train),(X_test,Y_test) = fashion_mnist.load_data()\n",
    "(X_train_mnist,Y_train_mnist),(X_test_mnist,Y_test_mnist) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bfe24e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_data(X_train,Y_train,X_test,Y_test):\n",
    "    \n",
    "    '''CHANGING THE SHAPE OF INPUT DATA'''\n",
    "    x_train=np.zeros((60000,784))\n",
    "    for i in range(X_train.shape[0]):\n",
    "        a=X_train[i].reshape(1,784)\n",
    "        x_train[i]=a\n",
    "        x_test=np.zeros((10000,784))\n",
    "    for i in range(X_test.shape[0]):\n",
    "        a=X_test[i].reshape(1,784)\n",
    "        x_test[i]=a\n",
    "    '''CONVERTING OUTPUT DATA INTO ONE HOT VECTOR FORM'''\n",
    "    a = np.max(Y_train)+1\n",
    "    y_train=np.zeros((Y_train.shape[0],a))\n",
    "    for i in range(Y_train.shape[0]):\n",
    "        for j in range(a):\n",
    "            if Y_train[i]==j:\n",
    "                y_train[i,j]=1\n",
    "    y_test=np.zeros((Y_test.shape[0],a))\n",
    "    for i in range(Y_test.shape[0]):\n",
    "        for j in range(a):\n",
    "            if Y_test[i]==j:\n",
    "                y_test[i,j]=1\n",
    "    '''CREATING VALIDATION DATA SET'''\n",
    "    x_train,x_val,y_train,y_val = train_test_split(x_train,y_train,test_size=0.10,random_state=10)\n",
    "    x_train=x_train.T/255\n",
    "    y_train=y_train.T\n",
    "    x_test=x_test.T/255\n",
    "    y_test=y_test.T\n",
    "    x_val=x_val.T/255\n",
    "    y_val=y_val.T\n",
    "    return x_train,y_train,x_val,y_val,x_test,y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "65d5200c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation(x):\n",
    "    '''DATA AUGMENTATION BY ADDING NOISE TO DATA'''\n",
    "    n,m=x.shape\n",
    "    mean=0\n",
    "    variance=0.001\n",
    "    std=np.sqrt(variance)\n",
    "    k=np.random.normal(loc=mean,scale=std,size=(n,m))\n",
    "    x=x+k\n",
    "    x=np.clip(x,0,1)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1897ff7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''ACTIVATION FUNCTIONS'''\n",
    "\n",
    "'''input: zl = w(l)*a(l-1) + b(l) where l is the lth Layer.The various activation functions implemented here are \n",
    "sigmoid,tanh,ReLu and Identity functions.'''\n",
    "\n",
    "\n",
    "#SIGMOID FUNCTION\n",
    "def sigmoid_function(z):\n",
    "    h=1./(1.+np.exp(-z))\n",
    "    \n",
    "    return h\n",
    "\n",
    "#TANH FUNCTION\n",
    "def tanh_function(z):\n",
    "    h=np.tanh(z)\n",
    "    \n",
    "    return h\n",
    "\n",
    "#RELU FUNCTION\n",
    "def relu_function(z):\n",
    "    h=np.maximum(z,0)\n",
    "    \n",
    "    return h\n",
    "    \n",
    "\n",
    "#IDENTITY FUNCTION\n",
    "def identity_function(z):\n",
    "    \n",
    "    return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b370971a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OUTPUT LAYER FUNCTION\n",
    "'''The given problem is a multi-class classification problem.So,we use softmax function for the output layer(L)\n",
    "    Z(L) = W(L)*A(L-1) + B(L) where Lth layer is the output layer.'''\n",
    "\n",
    "\n",
    "#SOFTMAX FUNCTION\n",
    "'''OUTPUT LAYER FUNCTION'''\n",
    "\n",
    "def softmax_function(z):\n",
    "    #z=z-np.max(z,axis=0,keepdims=True)\n",
    "    h = np.exp(z)/np.sum(np.exp(z), axis=0)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0ff00b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''DERIVATIVE FUNCTIONS'''\n",
    "\n",
    "'''These are the derivatives of the corresponding activation functions, which is used in backpropagation to find\n",
    "the derivative of activation functions.'''\n",
    "\n",
    "#DERIVATIVE OF SIGMOID FUNCTION\n",
    "def sigmoid_function_dash(z):\n",
    "    h = sigmoid_function(z)\n",
    "    \n",
    "    return h*(1-h)\n",
    "\n",
    "\n",
    "#DERIVATIVE OF TANH FUNCTION\n",
    "def tanh_function_dash(z):\n",
    "    h=tanh_function(z)\n",
    "    \n",
    "    return 1-(h)**2\n",
    "\n",
    "\n",
    "#DERIVATIVE OF RELU FUNCTION\n",
    "def relu_function_dash(z):\n",
    "    return 1*(z>0)\n",
    "    \n",
    "#DERIVATIVE OF IDENTITY FUNCTION\n",
    "def identity_function_dash(z):\n",
    "    h = identity_function(z)\n",
    "    \n",
    "    return np.ones(z.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "83307593",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''SOFTMAX DERIVATIVE'''\n",
    "def softmax_dash(Z):\n",
    "    h= softmax(z) * (1-softmax(z))\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9e8b063f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CROSS ENTROPY FUNCTION(DERIVATIVE OF OUTPUT LAYER)\n",
    "def cross_entropy_function(y,ycap,w,lambd):\n",
    "    '''This function is called as categorical cross entropy function.\n",
    "       input: Y:actual value of output\n",
    "              YCAP:predicted value of output\n",
    "              lambd:Regularisation parameter(L2 Rregularization is used here)'''\n",
    "    \n",
    "    ycap = np.clip(ycap, 1e-12, 1.0 - 1e-12)\n",
    "    m=y.shape[1]\n",
    "    cost=-(1/m)*np.sum(y*np.log(ycap))\n",
    "    regularization_cost=0\n",
    "    for i in range(len(w)):\n",
    "        regularization_cost += (lambd/(2*m))*np.sum(np.square(w[i]))\n",
    "        \n",
    "    return cost+regularization_cost\n",
    "     \n",
    "        \n",
    "#MEAN SQUARED ERROR FUNCTION\n",
    "def mean_squared_error_function(y,ycap,w,lambd):\n",
    "    '''input: Y:actual value of output\n",
    "              YCAP:predicted value of output\n",
    "              lambd:Regularisation parameter(L2 Rregularization is used here)'''   \n",
    "    ycap = np.clip(ycap, 1e-12, 1.0 - 1e-12)\n",
    "    m = y.shape[1]\n",
    "    mean_square_error = (1/m)*np.sum((y-ycap)**2)\n",
    "    reg_cost=0\n",
    "    for i in range(len(w)):\n",
    "        reg_cost += (lambd/(2*m))*np.sum(w[i]**2)\n",
    "    return mean_square_error + reg_cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d8c4bdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INITIALISE PARAMETERS\n",
    "\n",
    "'''input:  Layer_attributes is a list consisting of number of \n",
    "    neurons in each layer. Here,input layer is considered as 0th Layer, output layer is considered as Lth layer\n",
    "    and the layers from 1 to (L-1) are considered as hidden layers.Therefore, layer-attributes consists of (L+1)\n",
    "    values. The methods used here to initialise the values of parameters are Random and Xavier Initialisations.'''\n",
    "\n",
    "def random_initialization(layer_attributes):\n",
    "    \n",
    "    L=len(layer_attributes)-1\n",
    "    W=[]\n",
    "    B=[]\n",
    "    np.random.seed(10)\n",
    "    for i in range(1,L+1):\n",
    "        weight_i = np.random.uniform(-1,1,(layer_attributes[i],layer_attributes[i-1]))\n",
    "        bias_i=np.zeros((layer_attributes[i],1))\n",
    "        W.append(weight_i)\n",
    "        B.append(bias_i)\n",
    "        \n",
    "    return W,B\n",
    "\n",
    "def xavier_initialization(layer_attributes):\n",
    "    \n",
    "    L=len(layer_attributes)-1\n",
    "    W=[]\n",
    "    B=[]\n",
    "    for i in range(1,L+1):\n",
    "        lim = np.sqrt(6/(i+(i-1)))\n",
    "        weight_i = np.random.uniform(-lim,lim,(layer_attributes[i],layer_attributes[i-1]))\n",
    "        bias_i=np.zeros((layer_attributes[i],1))\n",
    "        W.append(weight_i)\n",
    "        B.append(bias_i)\n",
    "        \n",
    "    return W,B\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a89328db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FORWARD PROPaGATION\n",
    "def forward_propagation(x,w,b,activation='sigmoid_function'):\n",
    "    \n",
    "    '''Forward propagation is used to find the predicted value of output and cost function by going forward,starting from \n",
    "    input layer until the output layer.We calculate the pre-activation and activation values and returns the latter after each\n",
    "    layer. The input parameters taken are input data set,weights and bias value, and activation function to be used where the \n",
    "    default is set as sigmoid function. Softmax function is used to find the values at the output layer.\n",
    "    Here,z is the linear part and a is the non-linear part(activation function) of a neuron.'''\n",
    "    A=[]\n",
    "    Z=[]\n",
    "    length=len(w)\n",
    "    #Hidden layers\n",
    "    A.append(x)\n",
    "    for i in range(length-1):\n",
    "        z_i=np.dot(w[i],A[-1])+b[i]\n",
    "        Z.append(z_i)\n",
    "        if activation =='sigmoid_function':\n",
    "            a_i = sigmoid_function(z_i)\n",
    "            A.append(a_i)\n",
    "        elif activation=='tanh_function':\n",
    "            a_i = tanh_function(z_i)\n",
    "            A.append(a_i)\n",
    "        elif activation == 'relu_function':\n",
    "            a_i = relu_function(z_i)\n",
    "            A.append(a_i)\n",
    "        elif activation == 'identity_function':\n",
    "            a_i = identity_function(z_i)\n",
    "            A.append(a_i)\n",
    "    #output layer\n",
    "    z_l = np.dot(w[-1],A[-1]) + b[-1]\n",
    "    a_l = softmax_function(z_l)\n",
    "    A.append(a_l)\n",
    "    Z.append(z_l)\n",
    "\n",
    "    return Z,A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "eb9e67bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BACK PROPAGATION\n",
    "def back_propagation(A,y,W,B,Z,lambd,activation='sigmoid_function',loss='cross_entropy_function'):\n",
    "    \n",
    "    '''Back propagation is used to find the derivatives of each weights and biases at each layers by starting \n",
    "    from the output layer and travelling backwards.We find the derivatives wrto ouput layer,wrto hidden layer and eventually\n",
    "    wrto weights and biases;dw=dJ/dw,db=dJ/db,dz=dJ/dz.'''\n",
    "    m=y.shape[1]\n",
    "    L=len(W)\n",
    "    dW=[]\n",
    "    dB=[]\n",
    "    dZ=[]\n",
    "    #Output Layer\n",
    "    if loss=='cross_entropy_function':\n",
    "        dZ.append(A[-1]-y)\n",
    "        dB.append((1/m)*np.sum(dZ[-1],axis=1,keepdims=True))\n",
    "        dW.append((1/m)*(np.dot(dZ[-1],A[-2].T))+(lambd/m)*W[-1])\n",
    "    elif loss=='mean_squared_error_function':\n",
    "        dZ.append((A[-1]-y)*A[-1]*(1-A[-1]))\n",
    "        dB.append((1/m)*np.sum(dZ[-1],axis=1,keepdims=True))\n",
    "        dW.append((1/m)*(np.dot(dZ[-1],A[-2].T))+(lambd/m)*W[-1])\n",
    "        \n",
    "   \n",
    "    #Hidden layers\n",
    "    l=L-1\n",
    "    while l >0:\n",
    "        if activation=='sigmoid_function':\n",
    "            dz_l = (1/m)*np.dot(W[l].T,dZ[-1])*sigmoid_function_dash(A[l])\n",
    "            db_l = (1/m)*np.sum(dz_l,axis=1,keepdims=True)\n",
    "            dw_l = (1/m)*np.dot(dz_l,A[l-1].T) + (lambd/m)*W[l-1]\n",
    "            dW.append(dw_l)\n",
    "            dB.append(db_l)\n",
    "            dZ.append(dz_l)\n",
    "        \n",
    "        elif activation == 'relu_function':\n",
    "            dz_l = (1/m)*np.dot(W[l].T,dZ[-1])*relu_function_dash(A[l])\n",
    "            db_l = (1/m)*np.sum(dz_l,axis=1,keepdims=True)\n",
    "            dw_l = (1/m)*np.dot(dz_l,A[l-1].T) + (lambd/m)*W[l-1]\n",
    "            dW.append(dw_l)\n",
    "            dB.append(db_l)\n",
    "            dZ.append(dz_l)\n",
    "            \n",
    "        elif activation=='tanh_function':\n",
    "            dz_l = (1/m)*np.dot(W[l].T,dZ[-1])*tanh_function_dash(A[l])\n",
    "            db_l = (1/m)*np.sum(dz_l,axis=1,keepdims=True)\n",
    "            dw_l = (1/m)*np.dot(dz_l,A[l-1].T) + (lambd/m)*W[l-1]\n",
    "            dW.append(dw_l)\n",
    "            dB.append(db_l)\n",
    "            dZ.append(dz_l)\n",
    "            \n",
    "        elif activation=='identity_function':\n",
    "            dz_l = (1/m)*np.dot(W[l].T,dZ[-1])*identity_function_dash(A[l])\n",
    "            db_l = (1/m)*np.sum(dz_l,axis=1,keepdims=True)\n",
    "            dw_l = (1/m)*np.dot(dz_l,A[l-1].T) + (lambd/m)*W[l-1]\n",
    "            dW.append(dw_l)\n",
    "            dB.append(db_l)\n",
    "            dZ.append(dz_l)\n",
    "            \n",
    "        l=l-1\n",
    "            \n",
    "    return dZ[::-1],dW[::-1],dB[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "733191ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTIMIZERS\n",
    "def gradient_descent(W,B,dW,dB,learning_rate):\n",
    "    \n",
    "    '''Mini batch,vanilla and stochastic gradient descents can be performed for this basic \n",
    "    gradient descent variant.'''\n",
    "    \n",
    "    alpha=learning_rate\n",
    "    length=len(W)\n",
    "    for i in range(length):\n",
    "        W[i] = W[i] - alpha*dW[i]\n",
    "        B[i] = B[i] - alpha*dB[i]\n",
    "    return W,B\n",
    "\n",
    "def momentum_gradient_descent(w,b,dw,db,learning_rate,momentum,update_w,update_b):\n",
    "    \n",
    "    '''Nesterov accelarated gradient descent can also be implemented as a special case of this Momentum gradient\n",
    "    descent  where we find wlookahead before calculating gradients '''\n",
    "    \n",
    "    for i in range(len(w)):  \n",
    "        update_w[i] = (momentum*update_w[i]) + dw[i]\n",
    "        update_b[i] = (momentum*update_b[i]) + db[i]\n",
    "        w[i] = w[i] - (learning_rate*update_w[i])\n",
    "        b[i] = b[i] - (learning_rate*update_b[i])\n",
    "    return w,b,update_w,update_b\n",
    "\n",
    "def rms_prop(w,b,dw,db,learning_rate,beta,epsilon,v_t_w,v_t_b):\n",
    "    \n",
    "    '''RMSProp is an adaptive learning algorithm and the hyperparamteters used \n",
    "    here are learning_rate(alpha) and beta.'''\n",
    "    \n",
    "    for i in range(len(w)):\n",
    "        v_t_w[i] = beta*v_t_w[i] + (1-beta)*((dw[i])**2)\n",
    "        v_t_b[i] = beta*v_t_b[i] + (1-beta)*((db[i])**2)\n",
    "        w[i] = w[i] - (learning_rate/(np.sqrt(v_t_w[i]+epsilon)))*dw[i]\n",
    "        b[i] = b[i] - (learning_rate/(np.sqrt(v_t_b[i]+epsilon)))*db[i]\n",
    "        return w,b,v_t_w,v_t_b\n",
    "\n",
    "\n",
    "def adam(w,b,dw,db,learning_rate,beta1,beta2,epsilon,m_w,m_b,v_w,v_b,t):\n",
    "    \n",
    "    '''Adam(Adaptive moments) is one of the widely used adaptive learning gradient descent variant and it takes \n",
    "    beta1,beta2,learning_rate(alpha) as hyperparameters and also a small value epsilon to avoid zero division\n",
    "    errors.'''\n",
    "    \n",
    "    m_w_cap=[]\n",
    "    m_b_cap=[]\n",
    "    v_w_cap=[]\n",
    "    v_b_cap=[]\n",
    "    for k in range(len(w)):\n",
    "        upd_w=np.zeros((w[k].shape))\n",
    "        upd_b=np.zeros((b[k].shape))\n",
    "        m_w_cap.append(upd_w)\n",
    "        m_b_cap.append(upd_b)\n",
    "        v_w_cap.append(upd_w)\n",
    "        v_b_cap.append(upd_b)\n",
    "        \n",
    "    for i in range(len(w)):\n",
    "        m_w[i]=beta1*m_w[i]+(1-beta1)*dw[i]\n",
    "        m_b[i]=beta1*m_b[i]+(1-beta1)*db[i]\n",
    "        v_w[i]=beta2*v_w[i]+(1-beta2)*(dw[i]**2)\n",
    "        v_b[i]=beta2*v_b[i]+(1-beta2)*(db[i]**2)\n",
    "        m_w_cap[i] = (1/(1-math.pow(beta1,t)))*m_w[i]\n",
    "        m_b_cap[i] = (1/(1-math.pow(beta1,t)))*m_b[i]\n",
    "        v_w_cap[i] = (1/(1-math.pow(beta2,t)))*v_w[i]\n",
    "        v_b_cap[i] = (1/(1-math.pow(beta2,t)))*v_b[i]\n",
    "        w[i] = w[i] - (learning_rate/(np.sqrt(v_w_cap[i]+epsilon)))*m_w_cap[i]\n",
    "        b[i] = b[i] - (learning_rate/(np.sqrt(v_b_cap[i]+epsilon)))*m_b_cap[i]\n",
    "        \n",
    "    return w,b,m_w,m_b,v_w,v_b\n",
    "\n",
    "def nadam(w,b,dw,db,learning_rate,beta1,beta2,epsilon,m_w,m_b,v_w,v_b,t):\n",
    "    \n",
    "    '''NAdam(Nesterov Adam) is the nesterov gradient variant of Adam'''\n",
    "    \n",
    "    m_w_cap=[]\n",
    "    m_b_cap=[]\n",
    "    v_w_cap=[]\n",
    "    v_b_cap=[]\n",
    "    for k in range(len(w)):\n",
    "        upd_w=np.zeros((w[k].shape))\n",
    "        upd_b=np.zeros((b[k].shape))\n",
    "        m_w_cap.append(upd_w)\n",
    "        m_b_cap.append(upd_b)\n",
    "        v_w_cap.append(upd_w)\n",
    "        v_b_cap.append(upd_b)\n",
    "        \n",
    "    for i in range(len(w)):\n",
    "        \n",
    "        m_w[i] = beta1*m_w[i] + (1-beta1)*dw[i]\n",
    "        m_b[i] = beta1*m_b[i] + (1-beta1)*db[i]\n",
    "        v_w[i] = beta2*v_w[i] + (1-beta2)*(dw[i])**2\n",
    "        v_b[i] = beta2*v_b[i] + (1-beta2)*(db[i])**2\n",
    "        m_w_cap[i] = (1/(1-beta1**t))*m_w[i]\n",
    "        m_b_cap[i] = (1/(1-beta1**t))*m_b[i]\n",
    "        v_w_cap[i] = (1/(1-beta2**t))*v_w[i]\n",
    "        v_b_cap[i] = (1/(1-beta2**t))*v_b[i]\n",
    "        w[i] = w[i]-(learning_rate/np.sqrt(v_w_cap[i]+epsilon))*(beta1*m_w_cap[i]+(1-beta1)*dw[i]/(1-beta1**t))\n",
    "        b[i] = b[i]-(learning_rate/np.sqrt(v_b_cap[i]+epsilon))*(beta1*m_b_cap[i]+(1-beta1)*db[i]/(1-beta1**t))\n",
    "        \n",
    "    return w,b,m_w,m_b,v_w,v_b\n",
    "        \n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a998a78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ACCURACY\n",
    "def accuracy(y,yout):\n",
    "    '''Function to find the accuracy taking y and ypred as input and returns accracy value.'''\n",
    "    yout=np.argmax(yout,axis=0)\n",
    "    y = np.argmax(y,axis=0)   \n",
    "    acc=np.mean(y==yout)*100\n",
    "    return acc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "50cd41ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCTION FOR PLOTS\n",
    "def plot_error(j_train, j_val):\n",
    "    plt.plot(list(range(len(j_train))), j_train, 'r', label=\"Train Loss\")\n",
    "    plt.plot(list(range(len(j_val))), j_val, 'lime', label=\"Validation Loss\")\n",
    "    plt.title(\"Training and Validation Loss vs No. of Epochs\", size=16)\n",
    "    plt.xlabel(\"No. of epochs\", size=16)\n",
    "    plt.ylabel(\"Loss\", size=16)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_accuracy(acc_train, acc_val):\n",
    "    plt.plot(list(range(len(acc_train))), acc_train, 'r', label=\"Train Accuracy\")\n",
    "    plt.plot(list(range(len(acc_val))), acc_val, 'lime', label=\"Validation Accuracy\")\n",
    "    plt.title(\"Training and Validation Accuracy vs No. of Epochs\", size=16)\n",
    "    plt.xlabel(\"No. of epochs\", size=16)\n",
    "    plt.ylabel(\"Accuracy\", size=16)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "14ecd3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    " #PREDICT FUNCTION\n",
    "def predict(x,y,w,b,lambd,activation):\n",
    "    '''This function is to predict the cost and accuracy values of the test data\n",
    "       input :  x(input)\n",
    "                y(output)\n",
    "                w,b(weights and biases)\n",
    "                lamb(regularisation parameter)\n",
    "                loss(loss function)\n",
    "                activation(activation function)'''\n",
    "    \n",
    "    z,a = forward_propagation(x_train,w,b,activation)\n",
    "    acc= accuracy(y,a[-1])\n",
    "    y_pred=np.argmax(a[-1],axis=0)\n",
    "    Y=np.argmax(y,axis=0)\n",
    "            \n",
    "    return y_pred,Y,acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1c255eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network(x_train,y_train,x_val,y_val,learning_rate = 0.001,momentum = 0.9,beta=0.9,beta1=0.9,beta2=0.99,epochs = 20,num_hidden_layers = 1,neurons=64,batch_size=16,epsilon=0.00000001,weight_init='random_initialization',\n",
    "                  activation='relu_function',loss='cross_entropy_function',optimizer='adam',lambd=0,wandb_project=\"CS6910_DL_ASS1\"):\n",
    " \n",
    "    wandb.init(project=wandb_project)\n",
    "    run_name= f'lr_{learning_rate}_acti_{activation}_w_in{weight_init}_opt_{optimizer}_epoch_{epochs}_num_hid_{num_hidden_layers}_loss_{loss}_batchsize_{batch_size}_neur_{neurons}_lam_{lambd}_momentum_{momentum}_beta_{beta}_beta1_{beta1}_beta2_{beta2}'\n",
    "    print(run_name)\n",
    "    \n",
    "    '''initializing required attributes'''\n",
    "    layer=[]\n",
    "    n,m=x_train.shape\n",
    "    J_train=[]\n",
    "    Accuracy_train=[]\n",
    "    J_val=[]\n",
    "    Accuracy_val=[]\n",
    "    acc_train=0\n",
    "    acc_val=0\n",
    "    layer.append(x_train.shape[0])\n",
    "    for i in range(num_hidden_layers):\n",
    "        layer.append(neurons)\n",
    "    layer.append(y_train.shape[0])\n",
    "    print(f'neuron configuration: {layer}')\n",
    "    if weight_init=='random_initialization':\n",
    "        w,b=random_initialization(layer)\n",
    "    elif weight_init=='xavier_initialization':\n",
    "        w,b=xavier_initialization(layer)\n",
    "    update_w=[]\n",
    "    update_b=[]\n",
    "    w_lookahead=[]\n",
    "    b_lookahead=[]\n",
    "    v_t_w=[]\n",
    "    v_t_b=[]\n",
    "    m_w=[]\n",
    "    m_b=[]\n",
    "    v_w=[]\n",
    "    v_b=[]\n",
    "    for k in range(len(w)):\n",
    "        upd_w=np.zeros((w[k].shape))\n",
    "        upd_b=np.zeros((b[k].shape))\n",
    "        update_w.append(upd_w)\n",
    "        w_lookahead.append(upd_w)\n",
    "        update_b.append(upd_b)\n",
    "        b_lookahead.append(upd_b)\n",
    "        v_t_w.append(upd_w)\n",
    "        v_t_b.append(upd_b)\n",
    "        m_w.append(upd_w)\n",
    "        m_b.append(upd_b)\n",
    "        v_w.append(upd_w)\n",
    "        v_b.append(upd_b)\n",
    "        \n",
    "    num_batches = x_train.shape[1]//batch_size\n",
    "    \n",
    "    if optimizer=='vanilla_gradient_descent':\n",
    "        \n",
    "        for j in range(epochs):\n",
    "            z,a = forward_propagation(x_train,w,b,activation)\n",
    "            dz,dw,db=back_propagation(a,y_train,w,b,z,lambd,activation,loss)\n",
    "            w,b=gradient_descent(w,b,dw,db,learning_rate)\n",
    "            if loss=='cross_entropy_function':\n",
    "                \n",
    "                cost_train=cross_entropy_function(y_train,a[-1],w,lambd)\n",
    "                J_train.append(cost_train)\n",
    "                acc_train = accuracy(y_train,a[-1])\n",
    "                Accuracy_train.append(acc_train)\n",
    "                z_val,a_val = forward_propagation(x_val,w,b,activation)\n",
    "                cost_val = cross_entropy_function(y_val,a_val[-1],w,lambd)\n",
    "                J_val.append(cost_val)\n",
    "                acc_val=accuracy(y_val,a_val[-1])\n",
    "                Accuracy_val.append(acc_val)\n",
    "            elif loss=='mean_squared_error_function':\n",
    "                \n",
    "                cost_train=mean_squared_error_function(y_train,a[-1],w,lambd)\n",
    "                J_train.append(cost_train)\n",
    "                acc_train = accuracy(y_train,a[-1])\n",
    "                Accuracy_train.append(acc_train)\n",
    "                z_val,a_val = forward_propagation(x_val,w,b,activation)\n",
    "                cost_val = mean_squared_error_function(y_val,a_val[-1],w,lambd)\n",
    "                J_val.append(cost_val)\n",
    "                acc_val = accuracy(y_val,a_val[-1])\n",
    "                Accuracy_val.append(acc_val)\n",
    "            wandb.log({\"accuracy_train\": acc_train, \"accuracy_validation\": acc_val, \"loss_train\": cost_train, \"cost_validation\": cost_val, 'epochs': j})\n",
    "            \n",
    "            if j%(epochs/10)==0:\n",
    "                print(f' \\n epoch:{j:4d}  Train error:  {J_train[-1]:8.2f}  Train accuracy: {Accuracy_train[-1]:8.2f} Val error: {J_val[-1]:8.2f} Val accuracy: {Accuracy_val[-1]:8.2f}')\n",
    "            \n",
    "    if optimizer=='stochastic_gradient_descent':\n",
    "        \n",
    "         for j in range(epochs):\n",
    "            for i in range(m):\n",
    "                x_data = x_train[:,i].reshape(x_train.shape[0],1)\n",
    "                y_data = y_train[:,i].reshape(y_train.shape[0],1)\n",
    "                z,a = forward_propagation(x_data,w,b,activation)\n",
    "                dz,dw,db=back_propagation(a,y_data,w,b,z,lambd,activation,loss)\n",
    "                w,b=gradient_descent(w,b,dw,db,learning_rate)\n",
    "            z,a = forward_propagation(x_train,w,b,activation)\n",
    "            cost_train=cross_entropy_function(y_train,a[-1],w,lambd)\n",
    "            J_train.append(cost_train)\n",
    "            acc_train = accuracy(y_train,a[-1])\n",
    "            Accuracy_train.append(acc_train)\n",
    "            z_val,a_val = forward_propagation(x_val,w,b,activation)\n",
    "            cost_val = cross_entropy_function(y_val,a_val[-1],w,lambd)\n",
    "            J_val.append(cost_val)\n",
    "            acc_val = accuracy(y_val,a_val[-1])\n",
    "            wandb.log({\"accuracy_train\": acc_train, \"accuracy_validation\": acc_val, \"loss_train\": cost_train, \"cost_validation\": cost_val, 'epochs': j})\n",
    "            Accuracy_val.append(acc_val)\n",
    "            if j%(epochs/10)==0:\n",
    "                print(f' \\n epoch:{j:4d}  Train error:  {J_train[-1]:8.2f}  Train accuracy: {Accuracy_train[-1]:8.2f} Val error: {J_val[-1]:8.2f} Val accuracy: {Accuracy_val[-1]:8.2f}')\n",
    "            \n",
    "    if optimizer=='momentum_gradient_descent':\n",
    "            \n",
    "        for j in range(epochs):\n",
    "            \n",
    "            for i in range(num_batches):\n",
    "                x_mb = x_train[:,i*batch_size:(i+1)*batch_size].reshape(x_train.shape[0],batch_size)\n",
    "                y_mb = y_train[:,i*batch_size:(i+1)*batch_size].reshape(y_train.shape[0],batch_size)\n",
    "                z,a = forward_propagation(x_mb,w,b,activation)\n",
    "                dz,dw,db=back_propagation(a,y_mb,w,b,z,lambd,activation,loss)\n",
    "                w,b,update_w,update_b = momentum_gradient_descent(w,b,dw,db,learning_rate,momentum,update_w,update_b)\n",
    "            if x_train.shape[1] % batch_size !=0:\n",
    "                x_last = x_train[:,num_batches*batch_size:]\n",
    "                y_last = y_train[:,num_batches*batch_size:]\n",
    "                z,a = forward_propagation(x_last,w,b,activation)\n",
    "                dz,dw,db=back_propagation(a,y_last,w,b,z,lambd,activation,loss)\n",
    "                w,b,update_w,update_b = momentum_gradient_descent(w,b,dw,db,learning_rate,momentum,update_w,update_b)\n",
    "            z,a = forward_propagation(x_train,w,b,activation)\n",
    "            if loss=='cross_entropy_function':\n",
    "                \n",
    "                cost_train=cross_entropy_function(y_train,a[-1],w,lambd)\n",
    "                J_train.append(cost_train)\n",
    "                acc_train = accuracy(y_train,a[-1])\n",
    "                Accuracy_train.append(acc_train)\n",
    "                z_val,a_val = forward_propagation(x_val,w,b,activation)\n",
    "                cost_val = cross_entropy_function(y_val,a_val[-1],w,lambd)\n",
    "                J_val.append(cost_val)\n",
    "                acc_val=accuracy(y_val,a_val[-1])\n",
    "                Accuracy_val.append(acc_val)\n",
    "            elif loss=='mean_squared_error_function':\n",
    "                \n",
    "                cost_train=mean_squared_error_function(y_train,a[-1],w,lambd)\n",
    "                J_train.append(cost_train)\n",
    "                acc_train = accuracy(y_train,a[-1])\n",
    "                Accuracy_train.append(acc_train)\n",
    "                z_val,a_val = forward_propagation(x_val,w,b,activation)\n",
    "                cost_val = mean_squared_error_function(y_val,a_val[-1],w,lambd)\n",
    "                J_val.append(cost_val)\n",
    "                acc_val = accuracy(y_val,a_val[-1])\n",
    "                Accuracy_val.append(acc_val)\n",
    "            wandb.log({\"accuracy_train\": acc_train, \"accuracy_validation\": acc_val, \"loss_train\": cost_train, \"cost_validation\": cost_val, 'epochs': j})\n",
    "            \n",
    "            if j%(epochs/10)==0:\n",
    "                print(f' \\n epoch:{j:4d}  Train error:  {J_train[-1]:8.2f}  Train accuracy: {Accuracy_train[-1]:8.2f} Val error: {J_val[-1]:8.2f} Val accuracy: {Accuracy_val[-1]:8.2f}')\n",
    "                \n",
    "                \n",
    "    if optimizer == 'nesterov_accelarated_gradient_descent':\n",
    "        \n",
    "        for j in range(epochs):\n",
    "            \n",
    "            for i in range(num_batches):\n",
    "                x_mb = x_train[:,i*batch_size:(i+1)*batch_size].reshape(x_train.shape[0],batch_size)\n",
    "                y_mb = y_train[:,i*batch_size:(i+1)*batch_size].reshape(y_train.shape[0],batch_size)\n",
    "                for i in range(len(w)):\n",
    "                    w_lookahead[i] = w[i]-momentum*update_w[i]\n",
    "                    b_lookahead[i] = b[i]-momentum*update_b[i]\n",
    "                z,a = forward_propagation(x_mb,w_lookahead,b_lookahead,activation)\n",
    "                dz,dw,db=back_propagation(a,y_mb,w_lookahead,b_lookahead,z,lambd,activation,loss)\n",
    "                w,b,update_w,update_b = momentum_gradient_descent(w,b,dw,db,learning_rate,momentum,update_w,update_b)\n",
    "            if x_train.shape[1] % batch_size !=0:\n",
    "                x_last = x_train[:,num_batches*batch_size:]\n",
    "                y_last = y_train[:,num_batches*batch_size:]\n",
    "                z,a = forward_propagation(x_last,w_lookahead,b_lookahead,activation)\n",
    "                dz,dw,db=back_propagation(a,y_last,w_lookahead,b_lookahead,z,lambd,activation,loss)\n",
    "                w,b,update_w,update_b = momentum_gradient_descent(w,b,dw,db,learning_rate,momentum,update_w,update_b)\n",
    "                \n",
    "            z,a = forward_propagation(x_train,w,b,activation)\n",
    "            if loss=='cross_entropy_function':\n",
    "                \n",
    "                cost_train=cross_entropy_function(y_train,a[-1],w,lambd)\n",
    "                J_train.append(cost_train)\n",
    "                acc_train = accuracy(y_train,a[-1])\n",
    "                Accuracy_train.append(acc_train)\n",
    "                z_val,a_val = forward_propagation(x_val,w,b,activation)\n",
    "                cost_val = cross_entropy_function(y_val,a_val[-1],w,lambd)\n",
    "                J_val.append(cost_val)\n",
    "                acc_val=accuracy(y_val,a_val[-1])\n",
    "                Accuracy_val.append(acc_val)\n",
    "            elif loss=='mean_squared_error_function':\n",
    "                \n",
    "                cost_train=mean_squared_error_function(y_train,a[-1],w,lambd)\n",
    "                J_train.append(cost_train)\n",
    "                acc_train = accuracy(y_train,a[-1])\n",
    "                Accuracy_train.append(acc_train)\n",
    "                z_val,a_val = forward_propagation(x_val,w,b,activation)\n",
    "                cost_val = mean_squared_error_function(y_val,a_val[-1],w,lambd)\n",
    "                J_val.append(cost_val)\n",
    "                acc_val = accuracy(y_val,a_val[-1])\n",
    "                Accuracy_val.append(acc_val)\n",
    "            wandb.log({\"accuracy_train\": acc_train, \"accuracy_validation\": acc_val, \"loss_train\": cost_train, \"cost_validation\": cost_val, 'epochs': j})\n",
    "            \n",
    "            if j%(epochs/10)==0:\n",
    "                print(f' \\n epoch:{j:4d}  Train error:  {J_train[-1]:8.2f}  Train accuracy: {Accuracy_train[-1]:8.2f} Val error: {J_val[-1]:8.2f} Val accuracy: {Accuracy_val[-1]:8.2f}')\n",
    "                \n",
    "                \n",
    "    if optimizer == 'rms_prop':\n",
    "            \n",
    "        for j in range(epochs):\n",
    "            \n",
    "            for i in range(num_batches):\n",
    "                x_mb = x_train[:,i*batch_size:(i+1)*batch_size].reshape(x_train.shape[0],batch_size)\n",
    "                y_mb = y_train[:,i*batch_size:(i+1)*batch_size].reshape(y_train.shape[0],batch_size)\n",
    "                z,a = forward_propagation(x_mb,w,b,activation)\n",
    "                dz,dw,db=back_propagation(a,y_mb,w,b,z,lambd,activation,loss)\n",
    "                w,b,v_t_w,v_t_b = rms_prop(w,b,dw,db,learning_rate,beta,epsilon,v_t_w,v_t_b)\n",
    "            if x_train.shape[1] % batch_size !=0:\n",
    "                x_last = x_train[:,num_batches*batch_size:]\n",
    "                y_last = y_train[:,num_batches*batch_size:]\n",
    "                z,a = forward_propagation(x_last,w,b,activation)\n",
    "                dz,dw,db=back_propagation(a,y_last,w,b,z,lambd,activation,loss)\n",
    "                w,b,v_t_w,v_t_b = rms_prop(w,b,dw,db,learning_rate,beta,epsilon,v_t_w,v_t_b)\n",
    "            z,a = forward_propagation(x_train,w,b,activation)\n",
    "            if loss=='cross_entropy_function':\n",
    "                \n",
    "                cost_train=cross_entropy_function(y_train,a[-1],w,lambd)\n",
    "                J_train.append(cost_train)\n",
    "                acc_train = accuracy(y_train,a[-1])\n",
    "                Accuracy_train.append(acc_train)\n",
    "                z_val,a_val = forward_propagation(x_val,w,b,activation)\n",
    "                cost_val = cross_entropy_function(y_val,a_val[-1],w,lambd)\n",
    "                J_val.append(cost_val)\n",
    "                acc_val=accuracy(y_val,a_val[-1])\n",
    "                Accuracy_val.append(acc_val)\n",
    "            elif loss=='mean_squared_error_function':\n",
    "                \n",
    "                cost_train=mean_squared_error_function(y_train,a[-1],w,lambd)\n",
    "                J_train.append(cost_train)\n",
    "                acc_train = accuracy(y_train,a[-1])\n",
    "                Accuracy_train.append(acc_train)\n",
    "                z_val,a_val = forward_propagation(x_val,w,b,activation)\n",
    "                cost_val = mean_squared_error_function(y_val,a_val[-1],w,lambd)\n",
    "                J_val.append(cost_val)\n",
    "                acc_val = accuracy(y_val,a_val[-1])\n",
    "                Accuracy_val.append(acc_val)\n",
    "            wandb.log({\"accuracy_train\": acc_train, \"accuracy_validation\": acc_val, \"loss_train\": cost_train, \"cost_validation\": cost_val, 'epochs': j})\n",
    "            \n",
    "            if j%(epochs/10)==0:\n",
    "                print(f' \\n epoch:{j:4d}  Train error:  {J_train[-1]:8.2f}  Train accuracy: {Accuracy_train[-1]:8.2f} Val error: {J_val[-1]:8.2f} Val accuracy: {Accuracy_val[-1]:8.2f}')\n",
    "                        \n",
    "                \n",
    "    if optimizer == 'adam':\n",
    "        for  j in range(epochs):\n",
    "            \n",
    "            for i in range(num_batches):\n",
    "                x_mb = x_train[:,i*batch_size:(i+1)*batch_size].reshape(x_train.shape[0],batch_size)\n",
    "                y_mb = y_train[:,i*batch_size:(i+1)*batch_size].reshape(y_train.shape[0],batch_size)\n",
    "                z,a = forward_propagation(x_mb,w,b,activation)\n",
    "                dz,dw,db=back_propagation(a,y_mb,w,b,z,lambd,activation,loss)\n",
    "                w,b,m_w,m_b,v_w,v_b = adam(w,b,dw,db,learning_rate,beta1,beta2,epsilon,m_w,m_b,v_w,v_b,j+1)\n",
    "            if x_train.shape[1] % batch_size !=0:\n",
    "                x_last = x_train[:,num_batches*batch_size:]\n",
    "                y_last = y_train[:,num_batches*batch_size:]\n",
    "                z,a = forward_propagation(x_last,w,b,activation)\n",
    "                dz,dw,db=back_propagation(a,y_last,w,b,z,lambd,activation,loss)\n",
    "                w,b,m_w,m_b,v_w,v_b = adam(w,b,dw,db,learning_rate,beta1,beta2,epsilon,m_w,m_b,v_w,v_b,j+1)\n",
    "            z,a=forward_propagation(x_train,w,b,activation)\n",
    "            if loss=='cross_entropy_function':\n",
    "                \n",
    "                cost_train=cross_entropy_function(y_train,a[-1],w,lambd)\n",
    "                J_train.append(cost_train)\n",
    "                acc_train = accuracy(y_train,a[-1])\n",
    "                Accuracy_train.append(acc_train)\n",
    "                z_val,a_val = forward_propagation(x_val,w,b,activation)\n",
    "                cost_val = cross_entropy_function(y_val,a_val[-1],w,lambd)\n",
    "                J_val.append(cost_val)\n",
    "                acc_val=accuracy(y_val,a_val[-1])\n",
    "                Accuracy_val.append(acc_val)\n",
    "            elif loss=='mean_squared_error_function':\n",
    "                \n",
    "                cost_train=mean_squared_error_function(y_train,a[-1],w,lambd)\n",
    "                J_train.append(cost_train)\n",
    "                acc_train = accuracy(y_train,a[-1])\n",
    "                Accuracy_train.append(acc_train)\n",
    "                z_val,a_val = forward_propagation(x_val,w,b,activation)\n",
    "                cost_val = mean_squared_error_function(y_val,a_val[-1],w,lambd)\n",
    "                J_val.append(cost_val)\n",
    "                acc_val = accuracy(y_val,a_val[-1])\n",
    "                Accuracy_val.append(acc_val)\n",
    "            wandb.log({\"accuracy_train\": acc_train, \"accuracy_validation\": acc_val, \"loss_train\": cost_train, \"cost_validation\": cost_val, 'epochs': j})\n",
    "            \n",
    "            if j%(epochs/10)==0:\n",
    "                print(f' \\n epoch:{j:4d}  Train error:  {J_train[-1]:8.2f}  Train accuracy: {Accuracy_train[-1]:8.2f} Val error: {J_val[-1]:8.2f} Val accuracy: {Accuracy_val[-1]:8.2f}')\n",
    "                \n",
    "                \n",
    "    if optimizer =='nadam':\n",
    "        for  j in range(epochs):\n",
    "            \n",
    "            for i in range(num_batches):\n",
    "                x_mb = x_train[:,i*batch_size:(i+1)*batch_size].reshape(x_train.shape[0],batch_size)\n",
    "                y_mb = y_train[:,i*batch_size:(i+1)*batch_size].reshape(y_train.shape[0],batch_size)\n",
    "                z,a = forward_propagation(x_mb,w,b,activation)\n",
    "                dz,dw,db=back_propagation(a,y_mb,w,b,z,lambd,activation,loss)\n",
    "                w,b,m_w,m_b,v_w,v_b = nadam(w,b,dw,db,learning_rate,beta1,beta2,epsilon,m_w,m_b,v_w,v_b,j+1)\n",
    "            if x_train.shape[1] % batch_size !=0:\n",
    "                x_last = x_train[:,num_batches*batch_size:]\n",
    "                y_last = y_train[:,num_batches*batch_size:]\n",
    "                z,a = forward_propagation(x_last,w,b,activation)\n",
    "                dz,dw,db=back_propagation(a,y_last,w,b,z,lambd,activation,loss)\n",
    "                w,b,m_w,m_b,v_w,v_b = nadam(w,b,dw,db,learning_rate,beta1,beta2,epsilon,m_w,m_b,v_w,v_b,j+1)\n",
    "            z,a=forward_propagation(x_train,w,b,activation)\n",
    "            \n",
    "            if loss=='cross_entropy_function':\n",
    "                \n",
    "                cost_train=cross_entropy_function(y_train,a[-1],w,lambd)\n",
    "                J_train.append(cost_train)\n",
    "                acc_train = accuracy(y_train,a[-1])\n",
    "                Accuracy_train.append(acc_train)\n",
    "                z_val,a_val = forward_propagation(x_val,w,b,activation)\n",
    "                cost_val = cross_entropy_function(y_val,a_val[-1],w,lambd)\n",
    "                J_val.append(cost_val)\n",
    "                acc_val=accuracy(y_val,a_val[-1])\n",
    "                Accuracy_val.append(acc_val)\n",
    "            elif loss=='mean_squared_error_function':\n",
    "                \n",
    "                cost_train=mean_squared_error_function(y_train,a[-1],w,lambd)\n",
    "                J_train.append(cost_train)\n",
    "                acc_train = accuracy(y_train,a[-1])\n",
    "                Accuracy_train.append(acc_train)\n",
    "                z_val,a_val = forward_propagation(x_val,w,b,activation)\n",
    "                cost_val = mean_squared_error_function(y_val,a_val[-1],w,lambd)\n",
    "                J_val.append(cost_val)\n",
    "                acc_val = accuracy(y_val,a_val[-1])\n",
    "                Accuracy_val.append(acc_val)\n",
    "            wandb.log({\"accuracy_train\": acc_train, \"accuracy_validation\": acc_val, \"loss_train\": cost_train, \"cost_validation\": cost_val, 'epochs': j})\n",
    "            \n",
    "            if j%(epochs/10)==0:\n",
    "                print(f' \\n epoch:{j:4d}  Train error:  {J_train[-1]:8.2f}  Train accuracy: {Accuracy_train[-1]:8.2f} Val error: {J_val[-1]:8.2f} Val accuracy: {Accuracy_val[-1]:8.2f}')\n",
    "        \n",
    "            \n",
    "            \n",
    "    plot_error(J_train,J_val)\n",
    "    plot_accuracy(Accuracy_train,Accuracy_val)\n",
    "    wandb.run.name = run_name\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddcf62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--wandb_project',default=\"CS6910_DL_ASS1\")\n",
    "    parser.add_argument('--data_set', help='FASHION-MNIST OR MNIST',default='fashion_mnist')\n",
    "    parser.add_argument('--learning_rate', type=float, default =0.001,\n",
    "                    help='initial learning rate for gradient descent')\n",
    "    parser.add_argument('--momentum', type=float, default=0.9)\n",
    "    parser.add_argument('--num_hidden_layers', type=int, default=1, help='number of hidden layers')\n",
    "    parser.add_argument('--neurons',type=int, default=64, help='number of neurons in hidden layers')\n",
    "    parser.add_argument('--activation', default='relu_function',help='activation function (tanh_function or sigmoid_function or relu_function or identity_function)')\n",
    "    parser.add_argument('--loss', default='cross_entropy_function',\n",
    "                    help='loss function (cross_entropy_function or mean_squared_error_function)')\n",
    "    parser.add_argument('--optimizer', default='adam',\n",
    "      help='optimizers (stochastic_gradient_descent or momentum_gradient_descent or nesterov_accelarated_gradient_descent or adam or nadam or rms_prop)')\n",
    "    parser.add_argument('--batch_size',type=int,default=16)\n",
    "    parser.add_argument('--beta', type=float, default=0.9)\n",
    "    parser.add_argument('--beta1', type=float, default=0.9)\n",
    "    parser.add_argument('--beta2', type=float, default=0.99)\n",
    "    parser.add_argument('--epochs', type=int, default=10)\n",
    "    parser.add_argument('--weight_init', default='random_initialization',\n",
    "                    help='weight initialization(random_initialization or xavier_initialization)')\n",
    "    parser.add_argument('--lambd', type=float, default=0)\n",
    "    parser.add_argument('--epsilon', type=float, default=0.00000001)\n",
    "\n",
    "    \n",
    "\n",
    "    args = parser.parse_args()\n",
    "    if args.data_set==\"fashion_mnist\":\n",
    "        x_train,y_train,x_val,y_val,x_test,y_test=pre_processing_data(X_train,Y_train,X_test,Y_test)\n",
    "        x_train=data_augmentation(x_train)\n",
    "        x_val=data_augmentation(x_val)\n",
    "        x_test=data_augmentation(x_test)\n",
    "    elif args.data_set==\"mnist\":\n",
    "        x_train,y_train,x_val,y_val,x_test,y_test=pre_processing_data(X_train_mnist,Y_train_mnist,X_test_mnist,Y_test_mnist)\n",
    "        x_train=data_augmentation(x_train)\n",
    "        x_val=data_augmentation(x_val)\n",
    "        x_test=data_augmentation(x_test)\n",
    "    w,b=neural_network(x_train,y_train,x_val,y_val,args.learning_rate,args.momentum,args.beta,args.beta1,args.beta2,args.epochs,args.num_hidden_layers,args.neurons,args.batch_size,args.epsilon,args.weight_init,\n",
    "                  args.activation,args.loss,args.optimizer,args.lambd,args.wandb_project)\n",
    "    \n",
    "    wandb.run.save()\n",
    "    wandb.run.finish()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136f7d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
