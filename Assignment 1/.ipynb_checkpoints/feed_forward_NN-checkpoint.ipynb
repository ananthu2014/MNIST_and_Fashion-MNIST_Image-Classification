{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32eebf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f1d0668",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''LOADING TRAIN AND TEST DATA SET'''\n",
    "(X_train,Y_train),(X_test,Y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d02d9aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input train data is : (60000, 28, 28)\n",
      "Shape of input test data is : (10000, 28, 28)\n",
      "Shape of output train data is : (60000,)\n",
      "Shape of output test data is : (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(f'Shape of input train data is : {X_train.shape}')\n",
    "print(f'Shape of input test data is : {X_test.shape}')\n",
    "print(f'Shape of output train data is : {Y_train.shape}')\n",
    "print(f'Shape of output test data is : {Y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14821b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The various categorical variables in the data are : ['T-shirt', 'Trouser', 'Pullover', 'Skirt', 'Overcoat', 'Sandal', 'Shirt', 'Sneakers', 'Bag', 'Ankle boot']\n"
     ]
    }
   ],
   "source": [
    "'''Finding image of each category'''\n",
    "categories=[]\n",
    "images_list=[]\n",
    "attributes={0:'T-shirt',1:'Trouser',2:'Pullover',3:'Skirt',4:'Overcoat',5:'Sandal',\n",
    "            6:'Shirt',7:'Sneakers',8:'Bag',9:'Ankle boot'}\n",
    "title=list(attributes.values())\n",
    "print(f'The various categorical variables in the data are : {title}')\n",
    "for i in range(10):\n",
    "    categories.append(i)\n",
    "for i in range(Y_train.shape[0]):\n",
    "               if Y_train[i]==categories[0]:\n",
    "                    images_list.append(X_train[i])\n",
    "                    del(categories[0])\n",
    "                    if len(categories)==0:\n",
    "                        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fb53d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data (60000, 784)\n",
      "Test data (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "'''CHANGING THE SHAPE OF INPUT DATA'''\n",
    "x_train=np.zeros((60000,784))\n",
    "for i in range(X_train.shape[0]):\n",
    "    a=X_train[i].reshape(1,784)\n",
    "    x_train[i]=a\n",
    "print('Train data',x_train.shape)\n",
    "x_test=np.zeros((10000,784))\n",
    "for i in range(X_test.shape[0]):\n",
    "    a=X_test[i].reshape(1,784)\n",
    "    x_test[i]=a\n",
    "print('Test data',x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3879212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data (60000, 10)\n",
      "Test data (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "'''CONVERTING OUTPUT DATA INTO ONE HOT VECTOR FORM'''\n",
    "a = np.max(Y_train)+1\n",
    "y_train=np.zeros((Y_train.shape[0],a))\n",
    "for i in range(Y_train.shape[0]):\n",
    "    for j in range(a):\n",
    "        if Y_train[i]==j:\n",
    "            y_train[i,j]=1\n",
    "print('Train data',y_train.shape)\n",
    "y_test=np.zeros((Y_test.shape[0],a))\n",
    "for i in range(Y_test.shape[0]):\n",
    "    for j in range(a):\n",
    "        if Y_test[i]==j:\n",
    "            y_test[i,j]=1\n",
    "#print(y_test[0,:])\n",
    "print('Test data',y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79a69aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of train data is:(784, 54000)\n",
      "The shape of test data is:(784, 10000)\n",
      "The shape of validation data is:(784, 6000)\n"
     ]
    }
   ],
   "source": [
    "'''CREATING VALIDATION DATA SET'''\n",
    "'''The input data is split into train and validation data where validation comprises of 10% of the data.'''\n",
    "\n",
    "x_train,x_val,y_train,y_val = train_test_split(x_train,y_train,test_size=0.10,random_state=10)\n",
    "x_train=x_train.T/255\n",
    "y_train=y_train.T\n",
    "x_test=x_test.T/255\n",
    "y_test=y_test.T\n",
    "x_val=x_val.T/255\n",
    "y_val=y_val.T\n",
    "print(f'The shape of train data is:{x_train.shape}')\n",
    "print(f'The shape of test data is:{x_test.shape}')\n",
    "print(f'The shape of validation data is:{x_val.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1897ff7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''ACTIVATION FUNCTIONS'''\n",
    "\n",
    "'''input: zl = w(l)*a(l-1) + b(l) where l is the lth Layer.The various activation functions implemented here are \n",
    "sigmoid,tanh,ReLu and Identity functions.'''\n",
    "\n",
    "\n",
    "#SIGMOID FUNCTION\n",
    "def sigmoid_function(z):\n",
    "    h=1./(1.+np.exp(-z))\n",
    "    \n",
    "    return h\n",
    "\n",
    "#TANH FUNCTION\n",
    "def tanh_function(z):\n",
    "    h=np.tanh(z)\n",
    "    \n",
    "    return h\n",
    "\n",
    "#RELU FUNCTION\n",
    "def relu_function(z):\n",
    "    h=np.maximum(z,0)\n",
    "    \n",
    "    return h\n",
    "    \n",
    "\n",
    "#IDENTITY FUNCTION\n",
    "def identity_function(z):\n",
    "    \n",
    "    return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b370971a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OUTPUT LAYER FUNCTION\n",
    "'''The given problem is a multi-class classification problem.So,we use softmax function for the output layer(L)\n",
    "    Z(L) = W(L)*A(L-1) + B(L) where Lth layer is the output layer.'''\n",
    "\n",
    "\n",
    "#SOFTMAX FUNCTION\n",
    "'''OUTPUT LAYER FUNCTION'''\n",
    "\n",
    "def softmax_function(z):\n",
    "    #z=z-np.max(z,axis=0,keepdims=True) # to avoid Nan or division by zero errors\n",
    "    h = np.exp(z)/np.sum(np.exp(z), axis=0)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e8b063f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CROSS ENTROPY FUNCTION(DERIVATIVE OF OUTPUT LAYER)\n",
    "def cross_entropy_function(y,ycap,w,lambd):\n",
    "    '''This function is called as categorical cross entropy function.\n",
    "       input: Y:actual value of output\n",
    "              YCAP:predicted value of output\n",
    "              lambd:Regularisation parameter(L2 Rregularization is used here)'''\n",
    "    \n",
    "    #ycap = np.clip(ycap, 1e-12, 1.0 - 1e-12) #to avoid Nan error \n",
    "    m=y.shape[1]\n",
    "    cost=-(1/m)*np.sum(y*np.log(ycap))\n",
    "    regularization_cost=0\n",
    "    for i in range(len(w)):\n",
    "        regularization_cost += (lambd/(2*m))*np.sum(np.square(w[i]))\n",
    "        \n",
    "    return cost+regularization_cost\n",
    "     \n",
    "        \n",
    "#MEAN SQUARED ERROR FUNCTION\n",
    "def mean_squared_error_function(y,ycap,w,lambd):\n",
    "    '''input: Y:actual value of output\n",
    "              YCAP:predicted value of output\n",
    "              lambd:Regularisation parameter(L2 Rregularization is used here)'''   \n",
    "    ycap = np.clip(ycap, 1e-12, 1.0 - 1e-12)\n",
    "    m = y.shape[1]\n",
    "    mean_square_error = (1/m)*np.sum((y-ycap)**2)\n",
    "    reg_cost=0\n",
    "    for i in range(len(w)):\n",
    "        reg_cost += (lambd/(2*m))*np.sum(w[i]**2)\n",
    "    return mean_square_error + reg_cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8c4bdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INITIALISE PARAMETERS\n",
    "\n",
    "'''input:  Layer_attributes is a list consisting of number of \n",
    "    neurons in each layer. Here,input layer is considered as 0th Layer, output layer is considered as Lth layer\n",
    "    and the layers from 1 to (L-1) are considered as hidden layers.Therefore, layer-attributes consists of (L+1)\n",
    "    values. The methods used here to initialise the values of parameters are Random and Xavier Initialisations.'''\n",
    "\n",
    "def random_initialization(layer_attributes):\n",
    "    \n",
    "    L=len(layer_attributes)-1\n",
    "    W=[]\n",
    "    B=[]\n",
    "    np.random.seed(10)\n",
    "    for i in range(1,L+1):\n",
    "        weight_i = np.random.uniform(-1,1,(layer_attributes[i],layer_attributes[i-1]))\n",
    "        bias_i=np.zeros((layer_attributes[i],1))\n",
    "        #bias_i=np.full((layer_attributes[i],1),0.01) # in case of relu function to avoid vanishing gradient\n",
    "        W.append(weight_i)\n",
    "        B.append(bias_i)\n",
    "        \n",
    "    return W,B\n",
    "\n",
    "def xavier_initialization(layer_attributes):\n",
    "    \n",
    "    L=len(layer_attributes)-1\n",
    "    W=[]\n",
    "    B=[]\n",
    "    for i in range(1,L+1):\n",
    "        lim = np.sqrt(6/(i+(i-1)))\n",
    "        weight_i = np.random.uniform(-lim,lim,(layer_attributes[i],layer_attributes[i-1]))\n",
    "        bias_i=np.zeros((layer_attributes[i],1))\n",
    "        W.append(weight_i)\n",
    "        B.append(bias_i)\n",
    "        \n",
    "    return W,B\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a89328db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FORWARD PROPAGATION\n",
    "def forward_propagation(x,w,b,activation='sigmoid_function'):\n",
    "    \n",
    "    '''Forward propagation is used to find the predicted value of output and cost function by going forward,starting from \n",
    "    input layer until the output layer.We calculate the pre-activation and activation values and returns the latter after each\n",
    "    layer. The input parameters taken are input data set,weights and bias value, and activation function to be used where the \n",
    "    default is set as sigmoid function. Softmax function is used to find the values at the output layer.\n",
    "    Here,z is the linear part and a is the non-linear part(activation function) of a neuron.'''\n",
    "    A=[]\n",
    "    Z=[]\n",
    "    length=len(w)\n",
    "    #Hidden layers\n",
    "    A.append(x)\n",
    "    for i in range(length-1):\n",
    "        z_i=np.dot(w[i],A[-1])+b[i]\n",
    "        Z.append(z_i)\n",
    "        if activation =='sigmoid_function':\n",
    "            a_i = sigmoid_function(z_i)\n",
    "            A.append(a_i)\n",
    "        elif activation=='tanh_function':\n",
    "            a_i = tanh_function(z_i)\n",
    "            A.append(a_i)\n",
    "        elif activation == 'relu_function':\n",
    "            a_i = relu_function(z_i)\n",
    "            A.append(a_i)\n",
    "        elif activation == 'identity_function':\n",
    "            a_i = identity_function(z_i)\n",
    "            A.append(a_i)\n",
    "    #output layer\n",
    "    z_l = np.dot(w[-1],A[-1]) + b[-1]\n",
    "    a_l = softmax_function(z_l)\n",
    "    A.append(a_l)\n",
    "    Z.append(z_l)\n",
    "\n",
    "    return Z,A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1394476e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y,yout):\n",
    "    '''Function to find the accuracy taking y and ypred as input and returns accracy value.'''\n",
    "    yout=np.argmax(yout,axis=0)\n",
    "    y = np.argmax(y,axis=0)   \n",
    "    acc=np.mean(y==yout)*100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14ecd3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    " #PREDICT FUNCTION\n",
    "def predict(x,y,w,b,lambd,activation):\n",
    "    '''This function is to predict the cost and accuracy values of the test data by applying forward propogation\n",
    "       input :  x(input)\n",
    "                y(output)\n",
    "                w,b(weights and biases)\n",
    "                lamb(regularisation parameter)\n",
    "                loss(loss function)\n",
    "                activation(activation function)'''\n",
    "    \n",
    "    z,a = forward_propagation(x_train,w,b,activation)\n",
    "    acc= accuracy(y,a[-1])\n",
    "    y_pred=np.argmax(a[-1],axis=0)\n",
    "    Y=np.argmax(y,axis=0)\n",
    "    cost_train=cross_entropy_function(y_train,a[-1],w,lambd)\n",
    "            \n",
    "    return acc,cost_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa1e9d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network(neurons,num_hidden_layers,weight_init='random_initialization'):\n",
    "    \n",
    "    layer=[]\n",
    "    n,m=x_train.shape\n",
    "    layer.append(x_train.shape[0])\n",
    "    for i in range(num_hidden_layers):\n",
    "        layer.append(neurons)\n",
    "    layer.append(y_train.shape[0])\n",
    "    print(f'neuron configuration: {layer}')\n",
    "    if weight_init=='random_initialization':\n",
    "        w,b=random_initialization(layer)\n",
    "    elif weight_init=='xavier_initialization':\n",
    "        w,b=xavier_initialization(layer)\n",
    "    return w,b\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5b84a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neuron configuration: [784, 8, 8, 10]\n"
     ]
    }
   ],
   "source": [
    "w,b=neural_network(8,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52026c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc,cost_train=predict(x_train,y_train,w,b,lambd=0,activation='sigmoid_function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c18fe559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current accuracy of train data using random weights:9.96111111111111\n"
     ]
    }
   ],
   "source": [
    "print(f'Current accuracy of train data using random weights:{acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "19f60ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current cost of train data using random weights:2.7509770074850888\n"
     ]
    }
   ],
   "source": [
    "print(f'Current cost of train data using random weights:{cost_train}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5b81d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
